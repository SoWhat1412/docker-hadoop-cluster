{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.100.34.19:9988 neo4j, testing w full data, around 75g\n",
    "\n",
    "https://stackoverflow.com/questions/44590284/number-of-executors-in-spark-local-mode\n",
    "\n",
    "in local mode spark UI shows one driver and all cores, and none of the executors\n",
    "\n",
    "\n",
    "- Neo4jExperiment(sc, 1200, 125000), running around ~4h and errors to (spark.driver.memory=1g)\n",
    "\n",
    "        org.apache.spark.graphx.lib.ConnectedComponents$.run(ConnectedComponents.scala:50)\n",
    "        \n",
    "        Name: org.apache.spark.SparkException\n",
    "        Message: Job aborted due to stage failure: Task 32 in stage 1291.0 failed 1 times, most recent failure: Lost task 32.0 in stage 1291.0 (TID 61352, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 457952 ms\n",
    "        \n",
    "        Exception in thread \"Executor task launch worker-82\" java.lang.OutOfMemoryError: Java heap space\n",
    "\n",
    "- Trying with spark.driver.memory=45g, just in case, although error was executor memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:19.715597Z",
     "start_time": "2019-04-30T08:22:18.176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:08:44.302972Z",
     "start_time": "2019-04-30T09:08:41.937Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SQLContext, DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.{RDD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting spark resources\n",
    "- https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors\n",
    "- https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster\n",
    "- https://medium.com/@thejasbabu/spark-under-the-hood-partition-d386aaaa26b7\n",
    "- https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors\n",
    "\n",
    "\n",
    "\n",
    "### executors\n",
    "    kg@bigdata-009>\n",
    "    CPU(s):       64\n",
    "    Mem:          252G\n",
    "    \n",
    "     <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "     <value>64960</value>\n",
    "\n",
    "     <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "     <value>360</value>\n",
    "     \n",
    "     <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "     <value>36</value>\n",
    "     \n",
    "- You can assign the number of cores per executor with `--executor-cores`\n",
    "- `yarn.nodemanager.resource.memory-mb` needs to be larger than `--executor-memory`\n",
    "- Setting `spark.executor.*` only applied to `yarn` and not local\n",
    "- seems better to have 3 or 4 executors per node, instead of 1 executors per node\n",
    "- on each node leave ~10% memory and cpu for yarn and other overheads\n",
    "- TODO: unsolved whether spark.executor.cores refers to core or vcore, can test by setting spark.executor.cores to be large number, eg 340, and if executor still can be allocated then it refers to vcore \n",
    "\n",
    "- yarn.scheduler.maximum-allocation-vcores controls the maximum vcores that any submitted job can request. yarn.nodemanager.resource.cpu-vcores, on the other hand, controls how many vcores can be scheduled on a particular NodeManager instance. \n",
    "\n",
    "### partitions\n",
    "- The recommend number of partitions is around 3 or 4 times the number of CPUs in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:27.594242Z",
     "start_time": "2019-04-30T08:22:24.957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark web UI: http://10.100.34.19:4040/ (or 4041...)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.executor.memory,45g)\n",
      "(spark.app.id,local-1556616271074)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.instances,8)\n",
      "(spark.driver.extraJavaOptions,-Xms1024M -Xmx4096M -Dlog4j.logLevel=info)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.driver.memory,45g)\n",
      "(spark.executor.cores,4)\n",
      "(spark.neo4j.bolt.user,neo4j)\n",
      "(spark.executor.port,9993)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.master,local[38])\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.app.name,local08)\n",
      "(spark.driver.port,9992)\n",
      "(spark.externalBlockStore.folderName,spark-8a7a6aa5-5965-4278-ab2b-8752eaf4bbf9)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.driver.blockManager.port,9990)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "// spark.driver.memory=45g. Is set in docker/conf/spark-defaults.conf\n",
    "val conf = new SparkConf().set(\"spark.app.name\",\"local08\").set(\"spark.master\",\"local[38]\").set(\"spark.executor.instances\",\"8\").set(\"spark.executor.cores\",\"4\").set(\"spark.executor.memory\",\"45g\")\n",
    "\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// val neo = Neo4j(sc)\n",
    "\n",
    "println(\"Spark web UI: \" + \"http://10.100.34.19:4040/ (or 4041...)\") //spark web UI\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:12:25.234554Z",
     "start_time": "2019-04-30T09:12:24.764Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pickleThis[A](sc:SparkContext, dataToSave:A, schemaString:String, savePath:String) : String = {\n",
    "    val dataToSaveReady = dataToSave match {\n",
    "      case dataToSave:RDD[Row] => dataToSave // val qq : RDD[Row] = loaded_df.rdd // the reverse transformation\n",
    "      case dataToSave:Array[Row] => sc.parallelize(dataToSave.toList) // val qq = loaded_df.collect // the reverse transformation\n",
    "// DOESNT WORK because it intercepts List[Array[Any]]\n",
    "// case dataToSave:List[Array[String]] => sc.parallelize(dataToSave.map(p => Row(p: _*))) \n",
    "      case dataToSave:List[Array[_]] => {\n",
    "          val tmp = dataToSave.map(row => row.map(el => el.toString))\n",
    "          sc.parallelize(tmp.map(p => Row(p: _*))) \n",
    "      }\n",
    "      case _ => throw new Exception(\"Wrong argument type\")\n",
    "    }\n",
    "\n",
    "  val schema =\n",
    "    StructType(\n",
    "      schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "  val dfToSave = sqlContext.createDataFrame(dataToSaveReady, schema)\n",
    "\n",
    "  dfToSave.write.mode(\"overwrite\").save(savePath)\n",
    "  println(\"data overwritten\")\n",
    "    \n",
    "  return s\"\"\"val loaded_df = sqlContext.read.load(\\\"$savePath\\\")\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:32.189089Z",
     "start_time": "2019-04-30T08:22:32.173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // val query = \"MATCH p=()-[r:Person_relation]->() RETURN count(r)\"\n",
    "// // val query = \"MATCH (n:Person) RETURN count(n)\"\n",
    "\n",
    "// val query = s\"\"\"MATCH (a) WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "// UNWIND temp AS label\n",
    "// RETURN label, SUM(tempCnt) AS cnt\"\"\"\n",
    "\n",
    "// val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "// val response = cursor.rows.toList\n",
    "\n",
    "// val label_counts = response.sortWith(_(1).asInstanceOf[Long] > _(1).asInstanceOf[Long])\n",
    "// pickleThis(sc, label_counts, \"label count_n\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:32.706232Z",
     "start_time": "2019-04-30T08:22:32.700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")\n",
    "// label_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:33.448856Z",
     "start_time": "2019-04-30T08:22:33.445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val label_rel_counts = label_counts.map( row => {\n",
    "//     val str = row(0)\n",
    "//     val query = s\"MATCH (n:$str)-[r]-() return COUNT(r)\"\n",
    "//     val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "//     val response = cursor.rows.toList\n",
    "//     row :+ response(0)(0)\n",
    "// })\n",
    "// pickleThis(sc, label_rel_counts, \"label count_n count_r\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:36.184369Z",
     "start_time": "2019-04-30T08:22:34.182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "var label_rel_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casting columns to the right types, since we didnt save it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:40.384625Z",
     "start_time": "2019-04-30T08:22:37.637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- count_n: integer (nullable = true)\n",
      " |-- count_r: integer (nullable = true)\n",
      "\n",
      "+-----------------+--------+---------+\n",
      "|            label| count_n|  count_r|\n",
      "+-----------------+--------+---------+\n",
      "|         HC_repay| 1963947|        0|\n",
      "|            Phone|22173769| 51227904|\n",
      "|      Mobilephone|18822274| 42658529|\n",
      "|          Address| 7976921| 18842999|\n",
      "|       HC_presona|    8279|  5504220|\n",
      "|     Bank_account| 1868242|  2801423|\n",
      "|        HC_client| 4112976| 70850813|\n",
      "|           Person|19342651|140069325|\n",
      "|      HC_contract| 2185195|  2180338|\n",
      "|          ID_card| 4194817|  9779910|\n",
      "|     HC_blacklist|   94850|        0|\n",
      "|         Landline| 3351495|  8569375|\n",
      "|          HC_role|     206|        0|\n",
      "|     HC_intopiece| 5308780| 80801782|\n",
      "|    HC_client_mes| 6216762| 10353541|\n",
      "|HC_intopiece_link|15040832|        0|\n",
      "|         HC_staff|  177505|   718550|\n",
      "|          Company| 2892023| 21315735|\n",
      "|            Email| 1137400|  1169363|\n",
      "|      HC_graylist|   55464|        0|\n",
      "+-----------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_n\", label_rel_counts_df(\"count_n\").cast(\"int\"))\n",
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_r\", label_rel_counts_df(\"count_r\").cast(\"int\"))\n",
    "label_rel_counts_df.printSchema\n",
    "label_rel_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:42.800777Z",
     "start_time": "2019-04-30T08:22:41.694Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "// class Neo4jExperiment\n",
    "import org.neo4j.spark.{Neo4j, Neo4jGraph}\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "\n",
    "class Neo4jExperiment(val sc:SparkContext, val partitions:Int=12, val batch:Int=125) {\n",
    "    val neo = Neo4j(sc)\n",
    "    var neo4j : Neo4j = null.asInstanceOf[Neo4j]\n",
    "    var graph : Graph[Long, String] = null.asInstanceOf[Graph[Long, String]]\n",
    "    var graphSaveThis : Graph[VertexId,String] = null.asInstanceOf[Graph[VertexId, String]]\n",
    "    \n",
    "    def get_count_load_time(label:String) : Double = {\n",
    "        println(\"label: \" ++ label)\n",
    "        println(\"partitions: \" ++ partitions.toString)\n",
    "        println(\"batch: \" ++ batch.toString)\n",
    "        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            neo4j = neo.rels(s\"MATCH (n:$label)-[r]-(m) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(partitions).batch(batch)\n",
    "            graph = neo4j.loadGraph[Long,String]\n",
    "            println(\"get_count_load_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    def get_count_n_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.vertices.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_r_time(label:String) : Double = {        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.edges.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_cc_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            // Find the connected components\n",
    "            graphSaveThis = graph.connectedComponents()\n",
    "            println(\"get_count_cc_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_save_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            val return_val = Neo4jGraph.saveGraph(sc, graphSaveThis, \"sdComponent\")\n",
    "            println(\"saveGraph: \" ++ return_val.toString) // how many nodes has been written\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:22:49.514619Z",
     "start_time": "2019-04-30T08:22:48.307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+----------+--------------+\n",
      "|        label| count_n|  count_r|batch_size|partition_size|\n",
      "+-------------+--------+---------+----------+--------------+\n",
      "|   HC_presona|    8279|  5504220|     10000|           551|\n",
      "|     HC_staff|  177505|   718550|     10000|            72|\n",
      "|        Email| 1137400|  1169363|     10000|           117|\n",
      "| Bank_account| 1868242|  2801423|     10000|           281|\n",
      "|  HC_contract| 2185195|  2180338|     10000|           219|\n",
      "|      Company| 2892023| 21315735|     10000|          2132|\n",
      "|     Landline| 3351495|  8569375|     10000|           857|\n",
      "|    HC_client| 4112976| 70850813|     10000|          7086|\n",
      "|      ID_card| 4194817|  9779910|     10000|           978|\n",
      "| HC_intopiece| 5308780| 80801782|     10000|          8081|\n",
      "|HC_client_mes| 6216762| 10353541|     10000|          1036|\n",
      "|      Address| 7976921| 18842999|     10000|          1885|\n",
      "|  Mobilephone|18822274| 42658529|     10000|          4266|\n",
      "|       Person|19342651|140069325|     10000|         14007|\n",
      "|        Phone|22173769| 51227904|     10000|          5123|\n",
      "+-------------+--------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select a batch_size small enough for spark.executor.memory=45g, apparently 125000 was too big\n",
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "val batch_size = 10000\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:28:08.964865Z",
     "start_time": "2019-04-30T08:22:51.159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: HC_staff\n",
      "partitions: 72\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "514692\n",
      "718550\n",
      "get_count_cc_time ending\n",
      "saveGraph: (514692,0)\n"
     ]
    }
   ],
   "source": [
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "// val batch_size = 10000 // defined in cell above\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "\n",
    "var count_load_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_n_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_r_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_cc_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_save_time:Seq[(String, Double)] = Seq.empty\n",
    "\n",
    "df.filter($\"label\" === \"HC_staff\").head(1).foreach(row => { // df.collect.foreach(row => {\n",
    "    val label = row(0).toString\n",
    "    val partition_size:Int = df.filter($\"label\" === s\"$label\").select(\"partition_size\").head.get(0).toString.toInt\n",
    "    val batch_size:Int = df.filter($\"label\" === s\"$label\").select(\"batch_size\").head.get(0).toString.toInt\n",
    "    \n",
    "    val neo4jExperiment = new Neo4jExperiment(sc, partition_size, batch_size) // new Neo4jExperiment(sc, 12, 125) // new Neo4jExperiment(sc, 1200, 125000)\n",
    "    count_load_time = count_load_time :+ (label, neo4jExperiment.get_count_load_time(label))\n",
    "    count_n_time = count_n_time :+ (label, neo4jExperiment.get_count_n_time(label))\n",
    "    count_r_time = count_r_time :+ (label, neo4jExperiment.get_count_r_time(label))\n",
    "    count_cc_time = count_cc_time :+ (label, neo4jExperiment.get_count_cc_time(label))\n",
    "    count_save_time = count_save_time :+ (label, neo4jExperiment.get_count_save_time(label))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:44:49.362130Z",
     "start_time": "2019-04-30T08:44:45.759Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "|        label| count_n|  count_r|batch_size|partition_size|count_load_time|count_n_time|count_r_time|count_cc_time|count_save_time|\n",
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "|   HC_presona|    8279|  5504220|     10000|           551|           null|        null|        null|         null|           null|\n",
      "|     HC_staff|  177505|   718550|     10000|            72|          0.058|       6.135|       1.848|      122.821|         25.818|\n",
      "|        Email| 1137400|  1169363|     10000|           117|           null|        null|        null|         null|           null|\n",
      "| Bank_account| 1868242|  2801423|     10000|           281|           null|        null|        null|         null|           null|\n",
      "|  HC_contract| 2185195|  2180338|     10000|           219|           null|        null|        null|         null|           null|\n",
      "|      Company| 2892023| 21315735|     10000|          2132|           null|        null|        null|         null|           null|\n",
      "|     Landline| 3351495|  8569375|     10000|           857|           null|        null|        null|         null|           null|\n",
      "|    HC_client| 4112976| 70850813|     10000|          7086|           null|        null|        null|         null|           null|\n",
      "|      ID_card| 4194817|  9779910|     10000|           978|           null|        null|        null|         null|           null|\n",
      "| HC_intopiece| 5308780| 80801782|     10000|          8081|           null|        null|        null|         null|           null|\n",
      "|HC_client_mes| 6216762| 10353541|     10000|          1036|           null|        null|        null|         null|           null|\n",
      "|      Address| 7976921| 18842999|     10000|          1885|           null|        null|        null|         null|           null|\n",
      "|  Mobilephone|18822274| 42658529|     10000|          4266|           null|        null|        null|         null|           null|\n",
      "|       Person|19342651|140069325|     10000|         14007|           null|        null|        null|         null|           null|\n",
      "|        Phone|22173769| 51227904|     10000|          5123|           null|        null|        null|         null|           null|\n",
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// putting results in the df\n",
    "val dftemp = count_load_time.toDF(\"label\",\"count_load_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_n_time.toDF(\"label\",\"count_n_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_r_time.toDF(\"label\",\"count_r_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_cc_time.toDF(\"label\",\"count_cc_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_save_time.toDF(\"label\",\"count_save_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:20:38.805758Z",
     "start_time": "2019-04-30T09:20:38.060Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local08_sdout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:23:16.924522Z",
     "start_time": "2019-04-30T09:23:15.911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- count_n: integer (nullable = true)\n",
      " |-- count_r: integer (nullable = true)\n",
      " |-- batch_size: integer (nullable = true)\n",
      " |-- partition_size: long (nullable = true)\n",
      " |-- count_load_time: double (nullable = true)\n",
      " |-- count_n_time: double (nullable = true)\n",
      " |-- count_r_time: double (nullable = true)\n",
      " |-- count_cc_time: double (nullable = true)\n",
      " |-- count_save_time: double (nullable = true)\n",
      "\n",
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "|        label| count_n|  count_r|batch_size|partition_size|count_load_time|count_n_time|count_r_time|count_cc_time|count_save_time|\n",
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "|    HC_client| 4112976| 70850813|     10000|          7086|           null|        null|        null|         null|           null|\n",
      "|      Address| 7976921| 18842999|     10000|          1885|           null|        null|        null|         null|           null|\n",
      "|       Person|19342651|140069325|     10000|         14007|           null|        null|        null|         null|           null|\n",
      "|        Phone|22173769| 51227904|     10000|          5123|           null|        null|        null|         null|           null|\n",
      "|        Email| 1137400|  1169363|     10000|           117|           null|        null|        null|         null|           null|\n",
      "|   HC_presona|    8279|  5504220|     10000|           551|           null|        null|        null|         null|           null|\n",
      "|     HC_staff|  177505|   718550|     10000|            72|          0.058|       6.135|       1.848|      122.821|         25.818|\n",
      "|  Mobilephone|18822274| 42658529|     10000|          4266|           null|        null|        null|         null|           null|\n",
      "| Bank_account| 1868242|  2801423|     10000|           281|           null|        null|        null|         null|           null|\n",
      "|      Company| 2892023| 21315735|     10000|          2132|           null|        null|        null|         null|           null|\n",
      "|HC_client_mes| 6216762| 10353541|     10000|          1036|           null|        null|        null|         null|           null|\n",
      "| HC_intopiece| 5308780| 80801782|     10000|          8081|           null|        null|        null|         null|           null|\n",
      "|     Landline| 3351495|  8569375|     10000|           857|           null|        null|        null|         null|           null|\n",
      "|      ID_card| 4194817|  9779910|     10000|           978|           null|        null|        null|         null|           null|\n",
      "|  HC_contract| 2185195|  2180338|     10000|           219|           null|        null|        null|         null|           null|\n",
      "+-------------+--------+---------+----------+--------------+---------------+------------+------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val local08_sdout = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local08_sdout.pickle\")\n",
    "local08_sdout.printSchema\n",
    "local08_sdout.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:41:11.666615Z",
     "start_time": "2019-04-15T10:41:10.757Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // pattern is using `id(n)` if node property is set to null, like example below\n",
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// // val g = Neo4jGraph.loadGraph(sc, \"Person\", Seq(\"Person_relation\"), \"Person\")\n",
    "// val graph = neo.pattern((\"Person\",null),(\"Person_relation\",null),(\"Person\",null)).partitions(40).batch(850000).loadGraph[Long,Long]\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")\n",
    "\n",
    "// g.vertices.take(5)\n",
    "\n",
    "// val udfSquared = udf((num: Int) => {\n",
    "//     num - 9999\n",
    "//   }\n",
    "// )\n",
    "// df.withColumn(\"count_n2\", udfSquared($\"count_n\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // Pickling example\n",
    "// label_counts.foreach(row => println(row.deep.mkString(\"\\t\\t\")))\n",
    "// val label_counts_casted:List[(String,String)] = label_counts.map(el => (el(0).toString,el(1).toString))\n",
    "// val label_counts_df = sc.parallelize(label_counts_casted,1).toDS().toDF()\n",
    "// label_counts_df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// // val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// label_counts_df.show\n",
    "// label_counts_df.collect.foreach(el => println(el(0), el(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// turning df to Array[Seq[String]] with null values\n",
    "// df.collect.map(row => row.toSeq.map(el => Option(el).getOrElse(\"null\").toString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
