{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as local11, trying with pagerank instead of connected components\n",
    "Assuming there is some problem with connected components as noted in here\n",
    "https://stackoverflow.com/questions/40266712/spark-graphx-scaling-connected-components\n",
    "etc\n",
    "\n",
    "\n",
    "Running with local[1] \n",
    "\n",
    "Running time ??? days\n",
    "\n",
    "local11 error is due to either, and is uncaught:\n",
    "- ???\n",
    "\n",
    "// Shuffle blocks(?) consumed 688 GB before crashing on `HC_client` connected components (batch_size=10000, partition_size=7086)\n",
    "\n",
    "Using `cd  /data/tmp/tmp-docker` and `du -h`\n",
    "\n",
    "// See `local11_neo4j_connection_count.log` Using `cpu_mem_log_plot.ipynb`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:03.932552Z",
     "start_time": "2019-06-04T02:20:03.266Z"
    }
   },
   "outputs": [],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:06.585895Z",
     "start_time": "2019-06-04T02:20:03.599Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SQLContext, DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.{RDD}\n",
    "import scala.util.control.NonFatal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting spark resources\n",
    "- https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors\n",
    "- https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster\n",
    "- https://medium.com/@thejasbabu/spark-under-the-hood-partition-d386aaaa26b7\n",
    "- https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors\n",
    "\n",
    "\n",
    "\n",
    "### executors\n",
    "    kg@bigdata-009>\n",
    "    CPU(s):       64\n",
    "    Mem:          252G\n",
    "    \n",
    "     <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "     <value>64960</value>\n",
    "\n",
    "     <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "     <value>360</value>\n",
    "     \n",
    "     <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "     <value>36</value>\n",
    "     \n",
    "- You can assign the number of cores per executor with `--executor-cores`\n",
    "- `yarn.nodemanager.resource.memory-mb` needs to be larger than `--executor-memory`\n",
    "- Setting `spark.executor.*` only applied to `yarn` and not local\n",
    "- seems better to have 3 or 4 executors per node, instead of 1 executors per node\n",
    "- on each node leave ~10% memory and cpu for yarn and other overheads\n",
    "- TODO: unsolved whether spark.executor.cores refers to core or vcore, can test by setting spark.executor.cores to be large number, eg 340, and if executor still can be allocated then it refers to vcore \n",
    "\n",
    "- yarn.scheduler.maximum-allocation-vcores controls the maximum vcores that any submitted job can request. yarn.nodemanager.resource.cpu-vcores, on the other hand, controls how many vcores can be scheduled on a particular NodeManager instance. \n",
    "\n",
    "### partitions\n",
    "- The recommend number of partitions is around 3 or 4 times the number of CPUs in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:10.418998Z",
     "start_time": "2019-06-04T02:20:07.215Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// sc.stop() // see kernel.json, see \"__TOREE_OPTS__\": \"--nosparkcontext\"\n",
    "// spark.driver.memory=45g. Is set in docker/conf/spark-defaults.conf\n",
    "val conf = new SparkConf().set(\"spark.app.name\",\"local11\").set(\"spark.master\",\"local[1]\").set(\"spark.executor.instances\",\"1\").set(\"spark.executor.cores\",\"1\").set(\"spark.executor.memory\",\"45g\")\n",
    "\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// val neo = Neo4j(sc)\n",
    "\n",
    "println(\"Spark web UI: \" + \"http://10.100.34.19:4040/ (or 4041...)\") //spark web UI\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:11.341866Z",
     "start_time": "2019-06-04T02:20:08.302Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pickleThis[A](sc:SparkContext, dataToSave:A, schemaString:String, savePath:String) : String = {\n",
    "    val dataToSaveReady = dataToSave match {\n",
    "      case dataToSave:RDD[Row] => dataToSave // val qq : RDD[Row] = loaded_df.rdd // the reverse transformation\n",
    "      case dataToSave:Array[Row] => sc.parallelize(dataToSave.toList) // val qq = loaded_df.collect // the reverse transformation\n",
    "// DOESNT WORK because it intercepts List[Array[Any]]\n",
    "// case dataToSave:List[Array[String]] => sc.parallelize(dataToSave.map(p => Row(p: _*))) \n",
    "      case dataToSave:List[Array[_]] => {\n",
    "          val tmp = dataToSave.map(row => row.map(el => el.toString))\n",
    "          sc.parallelize(tmp.map(p => Row(p: _*))) \n",
    "      }\n",
    "      case _ => throw new Exception(\"Wrong argument type\")\n",
    "    }\n",
    "\n",
    "  val schema =\n",
    "    StructType(\n",
    "      schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "  val dfToSave = sqlContext.createDataFrame(dataToSaveReady, schema)\n",
    "\n",
    "  dfToSave.write.mode(\"overwrite\").save(savePath)\n",
    "  println(\"data overwritten\")\n",
    "    \n",
    "  return s\"\"\"val loaded_df = sqlContext.read.load(\\\"$savePath\\\")\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:11.361381Z",
     "start_time": "2019-06-04T02:20:09.104Z"
    }
   },
   "outputs": [],
   "source": [
    "// // val query = \"MATCH p=()-[r:Person_relation]->() RETURN count(r)\"\n",
    "// // val query = \"MATCH (n:Person) RETURN count(n)\"\n",
    "\n",
    "// val query = s\"\"\"MATCH (a) WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "// UNWIND temp AS label\n",
    "// RETURN label, SUM(tempCnt) AS cnt\"\"\"\n",
    "\n",
    "// val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "// val response = cursor.rows.toList\n",
    "\n",
    "// val label_counts = response.sortWith(_(1).asInstanceOf[Long] > _(1).asInstanceOf[Long])\n",
    "// pickleThis(sc, label_counts, \"label count_n\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:11.371599Z",
     "start_time": "2019-06-04T02:20:09.799Z"
    }
   },
   "outputs": [],
   "source": [
    "// val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")\n",
    "// label_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:11.407287Z",
     "start_time": "2019-06-04T02:20:11.247Z"
    }
   },
   "outputs": [],
   "source": [
    "// val label_rel_counts = label_counts.map( row => {\n",
    "//     val str = row(0)\n",
    "//     val query = s\"MATCH (n:$str)-[r]-() return COUNT(r)\"\n",
    "//     val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "//     val response = cursor.rows.toList\n",
    "//     row :+ response(0)(0)\n",
    "// })\n",
    "// pickleThis(sc, label_rel_counts, \"label count_n count_r\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:14.048540Z",
     "start_time": "2019-06-04T02:20:12.111Z"
    }
   },
   "outputs": [],
   "source": [
    "var label_rel_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casting columns to the right types, since we didnt save it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:20:16.411225Z",
     "start_time": "2019-06-04T02:20:13.543Z"
    }
   },
   "outputs": [],
   "source": [
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_n\", label_rel_counts_df(\"count_n\").cast(\"int\"))\n",
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_r\", label_rel_counts_df(\"count_r\").cast(\"int\"))\n",
    "label_rel_counts_df.printSchema\n",
    "label_rel_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T10:21:44.260148Z",
     "start_time": "2019-06-04T02:21:43.469Z"
    }
   },
   "outputs": [],
   "source": [
    "// val neo = Neo4j(sc)\n",
    "// var neo4j : Neo4j = null.asInstanceOf[Neo4j]\n",
    "// var graph : Graph[Long, String] = null.asInstanceOf[Graph[Long, String]]\n",
    "// var graphSaveThis : Graph[VertexId,String] = null.asInstanceOf[Graph[VertexId, String]]\n",
    "\n",
    "// neo4j = neo.rels(s\"MATCH (n:Address)-[r]-(m) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(10).batch(20)\n",
    "// graph = neo4j.loadGraph[Long,String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:34:14.889164Z",
     "start_time": "2019-05-07T09:34:13.785Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "// class Neo4jExperiment\n",
    "import org.neo4j.spark.{Neo4j, Neo4jGraph}\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "import org.apache.spark.graphx.lib.{PageRank}\n",
    "\n",
    "\n",
    "class Neo4jExperiment(val sc:SparkContext, val partitions:Int=12, val batch:Int=125) {\n",
    "    val neo = Neo4j(sc)\n",
    "    var neo4j : Neo4j = null.asInstanceOf[Neo4j]\n",
    "    var graph : Graph[Long, String] = null.asInstanceOf[Graph[Long, String]]\n",
    "//     var graphSaveThis : Graph[VertexId,String] = null.asInstanceOf[Graph[VertexId, String]]\n",
    "    var graphSaveThis : Graph[Double,Double] = null.asInstanceOf[Graph[Double, Double]]\n",
    "    \n",
    "    def get_count_load_time(label:String) : Double = {\n",
    "        println(\"label: \" ++ label)\n",
    "        println(\"partitions: \" ++ partitions.toString)\n",
    "        println(\"batch: \" ++ batch.toString)\n",
    "        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            neo4j = neo.rels(s\"MATCH (n:$label)-[r]-(m) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(partitions).batch(batch)\n",
    "            graph = neo4j.loadGraph[Long,String]\n",
    "            println(\"get_count_load_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case NonFatal(e) => {\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    def get_count_n_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.vertices.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case NonFatal(e) => {\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_r_time(label:String) : Double = {        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.edges.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case NonFatal(e) => {\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_cc_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            // Find pagerank, 5 iterations\n",
    "            graphSaveThis = PageRank.run(graph,5)\n",
    "            println(\"get_count_cc_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case NonFatal(e) => {\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_save_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            val return_val = Neo4jGraph.saveGraph(sc, graphSaveThis, \"sdComponent\")\n",
    "            println(\"saveGraph: \" ++ return_val.toString) // how many nodes has been written\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case NonFatal(e) => {\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:34:17.152416Z",
     "start_time": "2019-05-07T09:34:16.113Z"
    }
   },
   "outputs": [],
   "source": [
    "// select a batch_size small enough for spark.executor.memory=45g, apparently 125000 was too big\n",
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "val batch_size = 10000\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "// val batch_size = 10000 // defined in cell above\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "\n",
    "var count_load_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_n_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_r_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_cc_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_save_time:Seq[(String, Double)] = Seq.empty\n",
    "\n",
    "df.collect.foreach(row => {\n",
    "    val label = row(0).toString\n",
    "    val partition_size:Int = df.filter($\"label\" === s\"$label\").select(\"partition_size\").head.get(0).toString.toInt\n",
    "    val batch_size:Int = df.filter($\"label\" === s\"$label\").select(\"batch_size\").head.get(0).toString.toInt\n",
    "    \n",
    "    val neo4jExperiment = new Neo4jExperiment(sc, partition_size, batch_size) // new Neo4jExperiment(sc, 12, 125) // new Neo4jExperiment(sc, 1200, 125000)\n",
    "    count_load_time = count_load_time :+ (label, neo4jExperiment.get_count_load_time(label))\n",
    "    count_n_time = count_n_time :+ (label, neo4jExperiment.get_count_n_time(label))\n",
    "    count_r_time = count_r_time :+ (label, neo4jExperiment.get_count_r_time(label))\n",
    "    count_cc_time = count_cc_time :+ (label, neo4jExperiment.get_count_cc_time(label))\n",
    "    count_save_time = count_save_time :+ (label, neo4jExperiment.get_count_save_time(label))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// putting results in the df\n",
    "val dftemp = count_load_time.toDF(\"label\",\"count_load_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_n_time.toDF(\"label\",\"count_n_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_r_time.toDF(\"label\",\"count_r_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_cc_time.toDF(\"label\",\"count_cc_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_save_time.toDF(\"label\",\"count_save_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local09_sdout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val local08_sdout = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local09_sdout.pickle\")\n",
    "local08_sdout.printSchema\n",
    "local08_sdout.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// // pattern is using `id(n)` if node property is set to null, like example below\n",
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// // val g = Neo4jGraph.loadGraph(sc, \"Person\", Seq(\"Person_relation\"), \"Person\")\n",
    "// val graph = neo.pattern((\"Person\",null),(\"Person_relation\",null),(\"Person\",null)).partitions(40).batch(850000).loadGraph[Long,Long]\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")\n",
    "\n",
    "// g.vertices.take(5)\n",
    "\n",
    "// val udfSquared = udf((num: Int) => {\n",
    "//     num - 9999\n",
    "//   }\n",
    "// )\n",
    "// df.withColumn(\"count_n2\", udfSquared($\"count_n\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// // Pickling example\n",
    "// label_counts.foreach(row => println(row.deep.mkString(\"\\t\\t\")))\n",
    "// val label_counts_casted:List[(String,String)] = label_counts.map(el => (el(0).toString,el(1).toString))\n",
    "// val label_counts_df = sc.parallelize(label_counts_casted,1).toDS().toDF()\n",
    "// label_counts_df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// // val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// label_counts_df.show\n",
    "// label_counts_df.collect.foreach(el => println(el(0), el(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// turning df to Array[Seq[String]] with null values\n",
    "// df.collect.map(row => row.toSeq.map(el => Option(el).getOrElse(\"null\").toString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
