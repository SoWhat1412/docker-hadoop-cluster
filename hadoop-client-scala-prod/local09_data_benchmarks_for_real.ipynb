{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.100.34.19:9988 neo4j, testing w full data, around 75g\n",
    "\n",
    "https://stackoverflow.com/questions/44590284/number-of-executors-in-spark-local-mode\n",
    "\n",
    "in local mode spark UI shows one driver and all cores, and none of the executors\n",
    "\n",
    "\n",
    "- Neo4jExperiment(sc, 1200, 125000), running around ~4h and errors to (spark.driver.memory=1g)\n",
    "\n",
    "        org.apache.spark.graphx.lib.ConnectedComponents$.run(ConnectedComponents.scala:50)\n",
    "        \n",
    "        Name: org.apache.spark.SparkException\n",
    "        Message: Job aborted due to stage failure: Task 32 in stage 1291.0 failed 1 times, most recent failure: Lost task 32.0 in stage 1291.0 (TID 61352, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 457952 ms\n",
    "        \n",
    "        Exception in thread \"Executor task launch worker-82\" java.lang.OutOfMemoryError: Java heap space\n",
    "\n",
    "- Trying with spark.driver.memory=45g, just in case, although error was executor memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SQLContext, DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.{RDD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting spark resources\n",
    "- https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors\n",
    "- https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster\n",
    "- https://medium.com/@thejasbabu/spark-under-the-hood-partition-d386aaaa26b7\n",
    "- https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors\n",
    "\n",
    "\n",
    "\n",
    "### executors\n",
    "    kg@bigdata-009>\n",
    "    CPU(s):       64\n",
    "    Mem:          252G\n",
    "    \n",
    "     <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "     <value>64960</value>\n",
    "\n",
    "     <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "     <value>360</value>\n",
    "     \n",
    "     <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "     <value>36</value>\n",
    "     \n",
    "- You can assign the number of cores per executor with `--executor-cores`\n",
    "- `yarn.nodemanager.resource.memory-mb` needs to be larger than `--executor-memory`\n",
    "- Setting `spark.executor.*` only applied to `yarn` and not local\n",
    "- seems better to have 3 or 4 executors per node, instead of 1 executors per node\n",
    "- on each node leave ~10% memory and cpu for yarn and other overheads\n",
    "- TODO: unsolved whether spark.executor.cores refers to core or vcore, can test by setting spark.executor.cores to be large number, eg 340, and if executor still can be allocated then it refers to vcore \n",
    "\n",
    "- yarn.scheduler.maximum-allocation-vcores controls the maximum vcores that any submitted job can request. yarn.nodemanager.resource.cpu-vcores, on the other hand, controls how many vcores can be scheduled on a particular NodeManager instance. \n",
    "\n",
    "### partitions\n",
    "- The recommend number of partitions is around 3 or 4 times the number of CPUs in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark web UI: http://10.100.34.19:4040/ (or 4041...)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.executor.memory,45g)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.app.name,local09)\n",
      "(spark.executor.id,driver)\n",
      "(spark.driver.extraJavaOptions,-Xms100G -Xmx400G -Dlog4j.logLevel=info)\n",
      "(spark.executor.instances,8)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.driver.memory,45g)\n",
      "(spark.executor.cores,4)\n",
      "(spark.neo4j.bolt.user,neo4j)\n",
      "(spark.app.id,local-1557129082804)\n",
      "(spark.executor.port,9993)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.master,local[38])\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.externalBlockStore.folderName,spark-46e067be-9a20-4359-bb7f-5440a4da998d)\n",
      "(spark.driver.port,9992)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.driver.blockManager.port,9990)\n"
     ]
    }
   ],
   "source": [
    "// sc.stop() // see kernel.json, see \"__TOREE_OPTS__\": \"--nosparkcontext\"\n",
    "// spark.driver.memory=45g. Is set in docker/conf/spark-defaults.conf\n",
    "val conf = new SparkConf().set(\"spark.app.name\",\"local09\").set(\"spark.master\",\"local[38]\").set(\"spark.executor.instances\",\"8\").set(\"spark.executor.cores\",\"4\").set(\"spark.executor.memory\",\"45g\")\n",
    "\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// val neo = Neo4j(sc)\n",
    "\n",
    "println(\"Spark web UI: \" + \"http://10.100.34.19:4040/ (or 4041...)\") //spark web UI\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pickleThis[A](sc:SparkContext, dataToSave:A, schemaString:String, savePath:String) : String = {\n",
    "    val dataToSaveReady = dataToSave match {\n",
    "      case dataToSave:RDD[Row] => dataToSave // val qq : RDD[Row] = loaded_df.rdd // the reverse transformation\n",
    "      case dataToSave:Array[Row] => sc.parallelize(dataToSave.toList) // val qq = loaded_df.collect // the reverse transformation\n",
    "// DOESNT WORK because it intercepts List[Array[Any]]\n",
    "// case dataToSave:List[Array[String]] => sc.parallelize(dataToSave.map(p => Row(p: _*))) \n",
    "      case dataToSave:List[Array[_]] => {\n",
    "          val tmp = dataToSave.map(row => row.map(el => el.toString))\n",
    "          sc.parallelize(tmp.map(p => Row(p: _*))) \n",
    "      }\n",
    "      case _ => throw new Exception(\"Wrong argument type\")\n",
    "    }\n",
    "\n",
    "  val schema =\n",
    "    StructType(\n",
    "      schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "  val dfToSave = sqlContext.createDataFrame(dataToSaveReady, schema)\n",
    "\n",
    "  dfToSave.write.mode(\"overwrite\").save(savePath)\n",
    "  println(\"data overwritten\")\n",
    "    \n",
    "  return s\"\"\"val loaded_df = sqlContext.read.load(\\\"$savePath\\\")\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // val query = \"MATCH p=()-[r:Person_relation]->() RETURN count(r)\"\n",
    "// // val query = \"MATCH (n:Person) RETURN count(n)\"\n",
    "\n",
    "// val query = s\"\"\"MATCH (a) WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "// UNWIND temp AS label\n",
    "// RETURN label, SUM(tempCnt) AS cnt\"\"\"\n",
    "\n",
    "// val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "// val response = cursor.rows.toList\n",
    "\n",
    "// val label_counts = response.sortWith(_(1).asInstanceOf[Long] > _(1).asInstanceOf[Long])\n",
    "// pickleThis(sc, label_counts, \"label count_n\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")\n",
    "// label_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val label_rel_counts = label_counts.map( row => {\n",
    "//     val str = row(0)\n",
    "//     val query = s\"MATCH (n:$str)-[r]-() return COUNT(r)\"\n",
    "//     val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "//     val response = cursor.rows.toList\n",
    "//     row :+ response(0)(0)\n",
    "// })\n",
    "// pickleThis(sc, label_rel_counts, \"label count_n count_r\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "var label_rel_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casting columns to the right types, since we didnt save it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- count_n: integer (nullable = true)\n",
      " |-- count_r: integer (nullable = true)\n",
      "\n",
      "+-----------------+--------+---------+\n",
      "|            label| count_n|  count_r|\n",
      "+-----------------+--------+---------+\n",
      "|         HC_repay| 1963947|        0|\n",
      "|            Phone|22173769| 51227904|\n",
      "|      Mobilephone|18822274| 42658529|\n",
      "|          Address| 7976921| 18842999|\n",
      "|       HC_presona|    8279|  5504220|\n",
      "|     Bank_account| 1868242|  2801423|\n",
      "|        HC_client| 4112976| 70850813|\n",
      "|           Person|19342651|140069325|\n",
      "|      HC_contract| 2185195|  2180338|\n",
      "|          ID_card| 4194817|  9779910|\n",
      "|     HC_blacklist|   94850|        0|\n",
      "|         Landline| 3351495|  8569375|\n",
      "|          HC_role|     206|        0|\n",
      "|     HC_intopiece| 5308780| 80801782|\n",
      "|    HC_client_mes| 6216762| 10353541|\n",
      "|HC_intopiece_link|15040832|        0|\n",
      "|         HC_staff|  177505|   718550|\n",
      "|          Company| 2892023| 21315735|\n",
      "|            Email| 1137400|  1169363|\n",
      "|      HC_graylist|   55464|        0|\n",
      "+-----------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_n\", label_rel_counts_df(\"count_n\").cast(\"int\"))\n",
    "label_rel_counts_df = label_rel_counts_df.withColumn(\"count_r\", label_rel_counts_df(\"count_r\").cast(\"int\"))\n",
    "label_rel_counts_df.printSchema\n",
    "label_rel_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "// class Neo4jExperiment\n",
    "import org.neo4j.spark.{Neo4j, Neo4jGraph}\n",
    "import org.apache.spark.graphx.{Graph, VertexId}\n",
    "\n",
    "class Neo4jExperiment(val sc:SparkContext, val partitions:Int=12, val batch:Int=125) {\n",
    "    val neo = Neo4j(sc)\n",
    "    var neo4j : Neo4j = null.asInstanceOf[Neo4j]\n",
    "    var graph : Graph[Long, String] = null.asInstanceOf[Graph[Long, String]]\n",
    "    var graphSaveThis : Graph[VertexId,String] = null.asInstanceOf[Graph[VertexId, String]]\n",
    "    \n",
    "    def get_count_load_time(label:String) : Double = {\n",
    "        println(\"label: \" ++ label)\n",
    "        println(\"partitions: \" ++ partitions.toString)\n",
    "        println(\"batch: \" ++ batch.toString)\n",
    "        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            neo4j = neo.rels(s\"MATCH (n:$label)-[r]-(m) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(partitions).batch(batch)\n",
    "            graph = neo4j.loadGraph[Long,String]\n",
    "            println(\"get_count_load_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_load_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    def get_count_n_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.vertices.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_n_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_r_time(label:String) : Double = {        \n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            println(graph.edges.count)\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_r_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_cc_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            // Find the connected components\n",
    "            graphSaveThis = graph.connectedComponents()\n",
    "            println(\"get_count_cc_time ending\")\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_cc_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def get_count_save_time(label:String) : Double = {\n",
    "        val t_start = System.currentTimeMillis()\n",
    "        try {\n",
    "            val return_val = Neo4jGraph.saveGraph(sc, graphSaveThis, \"sdComponent\")\n",
    "            println(\"saveGraph: \" ++ return_val.toString) // how many nodes has been written\n",
    "            (System.currentTimeMillis() - t_start)/1000d\n",
    "        } catch {\n",
    "          case e: Exception => {\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERRORSTART\")\n",
    "              println(\"==\" * 20)\n",
    "              e.printStackTrace\n",
    "              println(\"==\" * 20 ++ \"get_count_save_time_SDERROREND\")\n",
    "              println(\"==\" * 20)\n",
    "              (System.currentTimeMillis() - t_start)/1000d + 1e8 // 1e8 seconds = 3 years\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+----------+--------------+\n",
      "|        label| count_n|  count_r|batch_size|partition_size|\n",
      "+-------------+--------+---------+----------+--------------+\n",
      "|   HC_presona|    8279|  5504220|     10000|           551|\n",
      "|     HC_staff|  177505|   718550|     10000|            72|\n",
      "|        Email| 1137400|  1169363|     10000|           117|\n",
      "| Bank_account| 1868242|  2801423|     10000|           281|\n",
      "|  HC_contract| 2185195|  2180338|     10000|           219|\n",
      "|      Company| 2892023| 21315735|     10000|          2132|\n",
      "|     Landline| 3351495|  8569375|     10000|           857|\n",
      "|    HC_client| 4112976| 70850813|     10000|          7086|\n",
      "|      ID_card| 4194817|  9779910|     10000|           978|\n",
      "| HC_intopiece| 5308780| 80801782|     10000|          8081|\n",
      "|HC_client_mes| 6216762| 10353541|     10000|          1036|\n",
      "|      Address| 7976921| 18842999|     10000|          1885|\n",
      "|  Mobilephone|18822274| 42658529|     10000|          4266|\n",
      "|       Person|19342651|140069325|     10000|         14007|\n",
      "|        Phone|22173769| 51227904|     10000|          5123|\n",
      "+-------------+--------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// select a batch_size small enough for spark.executor.memory=45g, apparently 125000 was too big\n",
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "val batch_size = 10000\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: HC_presona\n",
      "partitions: 551\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "5490054\n",
      "5504220\n",
      "get_count_cc_time ending\n",
      "saveGraph: (5490054,0)\n",
      "label: HC_staff\n",
      "partitions: 72\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "514692\n",
      "718550\n",
      "get_count_cc_time ending\n",
      "saveGraph: (514692,0)\n",
      "label: Email\n",
      "partitions: 117\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "2285911\n",
      "1169363\n",
      "get_count_cc_time ending\n",
      "saveGraph: (2285911,0)\n",
      "label: Bank_account\n",
      "partitions: 281\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "3643679\n",
      "2801423\n",
      "get_count_cc_time ending\n",
      "saveGraph: (3643679,0)\n",
      "label: HC_contract\n",
      "partitions: 219\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "4360674\n",
      "2180338\n",
      "get_count_cc_time ending\n",
      "saveGraph: (4360674,0)\n",
      "label: Company\n",
      "partitions: 2132\n",
      "batch: 10000\n",
      "get_count_load_time ending\n",
      "22985802\n",
      "21315735\n",
      "========================================get_count_cc_time_SDERRORSTART\n",
      "========================================\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 94 in stage 75904.0 failed 1 times, most recent failure: Lost task 94.0 in stage 75904.0 (TID 610966, localhost): java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:319)\n",
      "\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:209)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1823)\n",
      "\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:661)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:72)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
      "\tat scala.Option.foreach(Option.scala:236)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
      "\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n",
      "\tat org.apache.spark.graphx.Pregel$.apply(Pregel.scala:143)\n",
      "\tat org.apache.spark.graphx.lib.ConnectedComponents$.run(ConnectedComponents.scala:50)\n",
      "\tat org.apache.spark.graphx.GraphOps.connectedComponents(GraphOps.scala:417)\n",
      "\tat $line67.$read$$iwC$$iwC$Neo4jExperiment.get_count_cc_time(<console>:91)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:75)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:66)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC.<init>(<console>:106)\n",
      "\tat $line79.$read$$iwC$$iwC.<init>(<console>:108)\n",
      "\tat $line79.$read$$iwC.<init>(<console>:110)\n",
      "\tat $line79.$read.<init>(<console>:112)\n",
      "\tat $line79.$read$.<init>(<console>:116)\n",
      "\tat $line79.$read$.<clinit>(<console>)\n",
      "\tat $line79.$eval$.<init>(<console>:7)\n",
      "\tat $line79.$eval$.<clinit>(<console>)\n",
      "\tat $line79.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
      "\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
      "\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:319)\n",
      "\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:209)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1823)\n",
      "\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:661)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:72)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
      "\t... 3 more\n",
      "========================================get_count_cc_time_SDERROREND\n",
      "========================================\n",
      "========================================get_count_save_time_SDERRORSTART\n",
      "========================================\n",
      "java.lang.NullPointerException\n",
      "\tat org.neo4j.spark.Neo4jGraph$.saveGraph(Neo4jGraph.scala:79)\n",
      "\tat $line67.$read$$iwC$$iwC$Neo4jExperiment.get_count_save_time(<console>:109)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:76)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:66)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n",
      "\tat $line79.$read$$iwC$$iwC$$iwC.<init>(<console>:106)\n",
      "\tat $line79.$read$$iwC$$iwC.<init>(<console>:108)\n",
      "\tat $line79.$read$$iwC.<init>(<console>:110)\n",
      "\tat $line79.$read.<init>(<console>:112)\n",
      "\tat $line79.$read$.<init>(<console>:116)\n",
      "\tat $line79.$read$.<clinit>(<console>)\n",
      "\tat $line79.$eval$.<init>(<console>:7)\n",
      "\tat $line79.$eval$.<clinit>(<console>)\n",
      "\tat $line79.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
      "\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
      "\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "========================================get_count_save_time_SDERROREND\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: error: \n",
       "     while compiling: <console>\n",
       "        during phase: jvm\n",
       "     library version: version 2.10.5\n",
       "    compiler version: version 2.10.5\n",
       "  reconstructed args: -classpath file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:file:/home/jovyan/work/hadoop-client-scala-prod/file:file:/usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:file:/usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:file:/usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:file:/usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:file:/usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:file:/usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:file:/usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:file:/usr/local/spark/jars-sd/slf4j-api-1.7.7.jar:file:/usr/local/spark/conf/:file:/usr/local/spark/lib/spark-assembly-1.6.1-hadoop2.6.0.jar:file:/usr/local/spark/lib/datanucleus-rdbms-3.2.9.jar:file:/usr/local/spark/lib/datanucleus-core-3.2.10.jar:file:/usr/local/spark/lib/datanucleus-api-jdo-3.2.6.jar:file:/usr/local/hadoop/etc/hadoop/:file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar:file:/usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar:file:/usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar:file:/usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar:file:/usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar:file:/usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar:file:/usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar:file:/usr/local/spark/jars-sd/scala-reflect-2.10.4.jar:file:/usr/local/spark/jars-sd/slf4j-api-1.7.7.jar\n",
       "\n",
       "  last tree to typer: term value \n",
       "              symbol: variable value in object $eval (flags: <mutable> <triedcooking> private[this])\n",
       "   symbol definition: private[this] var value: Throwable\n",
       "                 tpe: <notype>\n",
       "       symbol owners: variable value -> object $eval -> package $line80\n",
       "      context owners: object $eval -> package $line80\n",
       "\n",
       "== Enclosing template or block ==\n",
       "\n",
       "Template( // val <local $eval>: <notype> in object $eval, tree.tpe=type\n",
       "  \"java.lang.Object\" // parents\n",
       "  ValDef(\n",
       "    private\n",
       "    \"_\"\n",
       "    <tpt>\n",
       "    <empty>\n",
       "  )\n",
       "  // 5 statements\n",
       "  ValDef( // private[this] var value: Throwable in object $eval\n",
       "    private <mutable> <local> <defaultinit> <triedcooking>\n",
       "    \"value \"\n",
       "    <tpt> // tree.tpe=Throwable\n",
       "    <empty>\n",
       "  )\n",
       "  DefDef( // def value(): Throwable in object $eval\n",
       "    <method> <accessor> <defaultinit> <triedcooking>\n",
       "    \"value\"\n",
       "    []\n",
       "    List(Nil)\n",
       "    <tpt> // tree.tpe=Throwable\n",
       "    $eval.this.\"value \" // private[this] var value: Throwable in object $eval, tree.tpe=Throwable\n",
       "  )\n",
       "  DefDef( // def value_=(x$1: Throwable): Unit in object $eval\n",
       "    <method> <accessor> <defaultinit> <triedcooking>\n",
       "    \"value_$eq\"\n",
       "    []\n",
       "    // 1 parameter list\n",
       "    ValDef( // x$1: Throwable\n",
       "      <param> <synthetic> <triedcooking>\n",
       "      \"x$1\"\n",
       "      <tpt> // tree.tpe=Throwable\n",
       "      <empty>\n",
       "    )\n",
       "    <tpt> // tree.tpe=Unit\n",
       "    Assign( // tree.tpe=Unit\n",
       "      $eval.this.\"value \" // private[this] var value: Throwable in object $eval, tree.tpe=Throwable\n",
       "      \"x$1\" // x$1: Throwable, tree.tpe=Throwable\n",
       "    )\n",
       "  )\n",
       "  DefDef( // def set(x: Object): Unit in object $eval\n",
       "    <method>\n",
       "    \"set\"\n",
       "    []\n",
       "    // 1 parameter list\n",
       "    ValDef( // x: Object\n",
       "      <param> <triedcooking>\n",
       "      \"x\"\n",
       "      <tpt> // tree.tpe=Object\n",
       "      <empty>\n",
       "    )\n",
       "    <tpt> // tree.tpe=Unit\n",
       "    Apply( // def value_=(x$1: Throwable): Unit in object $eval, tree.tpe=Unit\n",
       "      $eval.this.\"value_$eq\" // def value_=(x$1: Throwable): Unit in object $eval, tree.tpe=(x$1: Throwable)Unit\n",
       "      Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class Object, tree.tpe=Throwable\n",
       "        TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class Object, tree.tpe=()Throwable\n",
       "          \"x\".\"$asInstanceOf\" // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0\n",
       "          <tpt> // tree.tpe=Throwable\n",
       "        )\n",
       "        Nil\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  DefDef( // def <init>(): type in object $eval\n",
       "    <method>\n",
       "    \"<init>\"\n",
       "    []\n",
       "    List(Nil)\n",
       "    <tpt> // tree.tpe=type\n",
       "    Block( // tree.tpe=Unit\n",
       "      Apply( // def <init>(): Object in class Object, tree.tpe=Object\n",
       "        $eval.super.\"<init>\" // def <init>(): Object in class Object, tree.tpe=()Object\n",
       "        Nil\n",
       "      )\n",
       "      ()\n",
       "    )\n",
       "  )\n",
       ")\n",
       "\n",
       "== Expanded type of tree ==\n",
       "\n",
       "<notype>\n",
       "\n",
       "uncaught exception during compilation: java.lang.AssertionError\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 76043.0 failed 1 times, most recent failure: Lost task 8.0 in stage 76043.0 (TID 611057, localhost): java.io.IOException: No space left on device\n",
       "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
       "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
       "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
       "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
       "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
       "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n",
       "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:151)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n",
       "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:150)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:128)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\tSuppressed: java.io.IOException: No space left on device\n",
       "\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
       "\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
       "\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
       "\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
       "\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n",
       "\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
       "\t\t... 11 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "\tat scala.Option.foreach(Option.scala:236)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)\n",
       "\tat org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n",
       "\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n",
       "\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n",
       "\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n",
       "\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n",
       "\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n",
       "\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)\n",
       "\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)\n",
       "\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n",
       "\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)\n",
       "\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1383)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:68)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:66)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n",
       "\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n",
       "\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n",
       "\tat $iwC$$iwC$$iwC.<init>(<console>:106)\n",
       "\tat $iwC$$iwC.<init>(<console>:108)\n",
       "\tat $iwC.<init>(<console>:110)\n",
       "\tat <init>(<console>:112)\n",
       "\tat .<init>(<console>:116)\n",
       "\tat .<clinit>(<console>)\n",
       "\tat .<init>(<console>:7)\n",
       "\tat .<clinit>(<console>)\n",
       "\tat $print(<console>)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: java.io.IOException: No space left on device\n",
       "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
       "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
       "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
       "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
       "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
       "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n",
       "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:151)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n",
       "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:150)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:128)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\t... 3 more\n",
       "\tSuppressed: java.io.IOException: No space left on device\n",
       "\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
       "\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
       "\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
       "\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
       "\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n",
       "\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
       "\t\t... 11 more\n",
       "StackTrace: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = label_rel_counts_df.filter($\"count_r\" > 0).sort($\"count_n\")   // $\"count_n\".desc\n",
    "// val batch_size = 10000 // defined in cell above\n",
    "df = df.withColumn(\"batch_size\", lit(batch_size) ).withColumn(\"partition_size\", ceil($\"count_r\"/batch_size) )\n",
    "\n",
    "var count_load_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_n_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_r_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_cc_time:Seq[(String, Double)] = Seq.empty\n",
    "var count_save_time:Seq[(String, Double)] = Seq.empty\n",
    "\n",
    "df.collect.foreach(row => {\n",
    "    val label = row(0).toString\n",
    "    val partition_size:Int = df.filter($\"label\" === s\"$label\").select(\"partition_size\").head.get(0).toString.toInt\n",
    "    val batch_size:Int = df.filter($\"label\" === s\"$label\").select(\"batch_size\").head.get(0).toString.toInt\n",
    "    \n",
    "    val neo4jExperiment = new Neo4jExperiment(sc, partition_size, batch_size) // new Neo4jExperiment(sc, 12, 125) // new Neo4jExperiment(sc, 1200, 125000)\n",
    "    count_load_time = count_load_time :+ (label, neo4jExperiment.get_count_load_time(label))\n",
    "    count_n_time = count_n_time :+ (label, neo4jExperiment.get_count_n_time(label))\n",
    "    count_r_time = count_r_time :+ (label, neo4jExperiment.get_count_r_time(label))\n",
    "    count_cc_time = count_cc_time :+ (label, neo4jExperiment.get_count_cc_time(label))\n",
    "    count_save_time = count_save_time :+ (label, neo4jExperiment.get_count_save_time(label))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// putting results in the df\n",
    "val dftemp = count_load_time.toDF(\"label\",\"count_load_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_n_time.toDF(\"label\",\"count_n_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_r_time.toDF(\"label\",\"count_r_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_cc_time.toDF(\"label\",\"count_cc_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "val dftemp = count_save_time.toDF(\"label\",\"count_save_time\")\n",
    "df = df.join(dftemp, Seq(\"label\"), \"left\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local09_sdout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val local08_sdout = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/local09_sdout.pickle\")\n",
    "local08_sdout.printSchema\n",
    "local08_sdout.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // pattern is using `id(n)` if node property is set to null, like example below\n",
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// // val g = Neo4jGraph.loadGraph(sc, \"Person\", Seq(\"Person_relation\"), \"Person\")\n",
    "// val graph = neo.pattern((\"Person\",null),(\"Person_relation\",null),(\"Person\",null)).partitions(40).batch(850000).loadGraph[Long,Long]\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")\n",
    "\n",
    "// g.vertices.take(5)\n",
    "\n",
    "// val udfSquared = udf((num: Int) => {\n",
    "//     num - 9999\n",
    "//   }\n",
    "// )\n",
    "// df.withColumn(\"count_n2\", udfSquared($\"count_n\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // Pickling example\n",
    "// label_counts.foreach(row => println(row.deep.mkString(\"\\t\\t\")))\n",
    "// val label_counts_casted:List[(String,String)] = label_counts.map(el => (el(0).toString,el(1).toString))\n",
    "// val label_counts_df = sc.parallelize(label_counts_casted,1).toDS().toDF()\n",
    "// label_counts_df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// // val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// label_counts_df.show\n",
    "// label_counts_df.collect.foreach(el => println(el(0), el(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// turning df to Array[Seq[String]] with null values\n",
    "// df.collect.map(row => row.toSeq.map(el => Option(el).getOrElse(\"null\").toString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
