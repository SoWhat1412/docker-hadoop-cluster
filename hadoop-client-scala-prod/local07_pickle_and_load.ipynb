{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.100.34.19:9988 neo4j, testing w full data, around 75g\n",
    "from 03, adjusting spark resources and testing with different amount of data\n",
    "\n",
    "https://stackoverflow.com/questions/44590284/number-of-executors-in-spark-local-mode\n",
    "\n",
    "in local mode spark UI shows one driver and all cores, and none of the executors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:48:42.321628Z",
     "start_time": "2019-04-26T09:48:41.521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:48:44.208026Z",
     "start_time": "2019-04-26T09:48:41.834Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SQLContext, Row, SaveMode}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.rdd.{RDD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting spark resources\n",
    "- https://stackoverflow.com/questions/29940711/apache-spark-setting-executor-instances-does-not-change-the-executors\n",
    "- https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster\n",
    "- https://medium.com/@thejasbabu/spark-under-the-hood-partition-d386aaaa26b7\n",
    "- https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors\n",
    "\n",
    "\n",
    "\n",
    "### executors\n",
    "    kg@bigdata-009>\n",
    "    CPU(s):       64\n",
    "    Mem:          252G\n",
    "    \n",
    "     <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "     <value>64960</value>\n",
    "\n",
    "     <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "     <value>360</value>\n",
    "     \n",
    "     <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "     <value>36</value>\n",
    "     \n",
    "- You can assign the number of cores per executor with `--executor-cores`\n",
    "- `yarn.nodemanager.resource.memory-mb` needs to be larger than `--executor-memory`\n",
    "- Setting `spark.executor.*` only applied to `yarn` and not local\n",
    "- seems better to have 3 or 4 executors per node, instead of 1 executors per node\n",
    "- on each node leave ~10% memory and cpu for yarn and other overheads\n",
    "- TODO: unsolved whether spark.executor.cores refers to core or vcore, can test by setting spark.executor.cores to be large number, eg 340, and if executor still can be allocated then it refers to vcore \n",
    "\n",
    "- yarn.scheduler.maximum-allocation-vcores controls the maximum vcores that any submitted job can request. yarn.nodemanager.resource.cpu-vcores, on the other hand, controls how many vcores can be scheduled on a particular NodeManager instance. \n",
    "\n",
    "### partitions\n",
    "- The recommend number of partitions is around 3 or 4 times the number of CPUs in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:48:50.620240Z",
     "start_time": "2019-04-26T09:48:47.634Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark web UI: http://10.100.34.19:4040/ (or 4041...)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.app.id,local-1556272128883)\n",
      "(spark.externalBlockStore.folderName,spark-6deb1a8a-a18e-4261-975f-a28e261d835b)\n",
      "(spark.driver.extraJavaOptions,-Xms1024M -Xmx4096M -Dlog4j.logLevel=info)\n",
      "(spark.driver.port,10002)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.executor.cores,4)\n",
      "(spark.neo4j.bolt.user,neo4j)\n",
      "(spark.executor.port,9993)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.master,local[40])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.driver.blockManager.port,9990)\n",
      "(spark.app.name,org.apache.toree.Main)\n",
      "(spark.executor.instances,9)\n",
      "(spark.executor.memory,50g)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "val conf = new SparkConf().set(\"spark.master\",\"local[40]\").set(\"spark.executor.instances\", \"9\").set(\"spark.executor.cores\", \"4\").set(\"spark.executor.memory\", \"50g\")\n",
    "  \n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val neo = Neo4j(sc)\n",
    "\n",
    "println(\"Spark web UI: \" + \"http://10.100.34.19:4040/ (or 4041...)\") //spark web UI\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:48:54.185119Z",
     "start_time": "2019-04-26T09:48:53.092Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pickleThis[A](sc:SparkContext, dataToSave:A, schemaString:String, savePath:String) : String = {\n",
    "    val dataToSaveReady = dataToSave match {\n",
    "      case dataToSave:RDD[Row] => dataToSave // val qq : RDD[Row] = loaded_df.rdd // the reverse transformation\n",
    "      case dataToSave:Array[Row] => sc.parallelize(dataToSave.toList) // val qq = loaded_df.collect // the reverse transformation\n",
    "// DOESNT WORK because it intercepts List[Array[Any]]\n",
    "// case dataToSave:List[Array[String]] => sc.parallelize(dataToSave.map(p => Row(p: _*))) \n",
    "      case dataToSave:List[Array[_]] => {\n",
    "          val tmp = dataToSave.map(row => row.map(el => el.toString))\n",
    "          sc.parallelize(tmp.map(p => Row(p: _*))) \n",
    "      }\n",
    "      case _ => throw new Exception(\"Wrong argument type\")\n",
    "    }\n",
    "\n",
    "  val schema =\n",
    "    StructType(\n",
    "      schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "  val dfToSave = sqlContext.createDataFrame(dataToSaveReady, schema)\n",
    "\n",
    "  dfToSave.write.mode(\"overwrite\").save(savePath)\n",
    "  println(\"data overwritten\")\n",
    "    \n",
    "  return s\"\"\"val loaded_df = sqlContext.read.load(\\\"$savePath\\\")\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:52:15.817029Z",
     "start_time": "2019-04-26T09:49:03.884Z"
    }
   },
   "outputs": [],
   "source": [
    "// val query = \"MATCH p=()-[r:Person_relation]->() RETURN count(r)\"\n",
    "// val query = \"MATCH (n:Person) RETURN count(n)\"\n",
    "\n",
    "val query = s\"\"\"MATCH (a) WITH DISTINCT LABELS(a) AS temp, COUNT(a) AS tempCnt\n",
    "UNWIND temp AS label\n",
    "RETURN label, SUM(tempCnt) AS cnt\"\"\"\n",
    "\n",
    "val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "val response = cursor.rows.toList\n",
    "\n",
    "val label_counts = response.sortWith(_(1).asInstanceOf[Long] > _(1).asInstanceOf[Long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T09:52:20.445039Z",
     "start_time": "2019-04-26T09:49:04.652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "data overwritten\n",
      "+-----------------+--------+\n",
      "|            label| count_n|\n",
      "+-----------------+--------+\n",
      "|    HC_client_mes| 6216762|\n",
      "|     HC_blacklist|   94850|\n",
      "|         HC_repay| 1963947|\n",
      "|     Bank_account| 1868242|\n",
      "|      HC_graylist|   55464|\n",
      "|          Address| 7976921|\n",
      "|           Person|19342651|\n",
      "|            Email| 1137400|\n",
      "|        HC_client| 4112976|\n",
      "|          Company| 2892023|\n",
      "|      HC_contract| 2185195|\n",
      "|         HC_staff|  177505|\n",
      "|HC_intopiece_link|15040832|\n",
      "|          HC_role|     206|\n",
      "|       HC_presona|    8279|\n",
      "|     HC_intopiece| 5308780|\n",
      "|      Mobilephone|18822274|\n",
      "|            Phone|22173769|\n",
      "|          ID_card| 4194817|\n",
      "|         Landline| 3351495|\n",
      "+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickleThis(sc, label_counts, \"label count_n\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")\n",
    "val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts.pickle\")\n",
    "label_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T07:57:28.551457Z",
     "start_time": "2019-04-23T07:57:27.901Z"
    }
   },
   "outputs": [],
   "source": [
    "// // Pickling example\n",
    "// label_counts.foreach(row => println(row.deep.mkString(\"\\t\\t\")))\n",
    "// val label_counts_casted:List[(String,String)] = label_counts.map(el => (el(0).toString,el(1).toString))\n",
    "// val label_counts_df = sc.parallelize(label_counts_casted,1).toDS().toDF()\n",
    "// label_counts_df.write.mode(\"overwrite\").save(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// // val label_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_counts_df.pickle\")\n",
    "// label_counts_df.show\n",
    "// label_counts_df.collect.foreach(el => println(el(0), el(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:52.027875Z",
     "start_time": "2019-04-26T09:52:29.012Z"
    }
   },
   "outputs": [],
   "source": [
    "val label_rel_counts = label_counts.map( row => {\n",
    "    val str = row(0)\n",
    "    val query = s\"MATCH (n:$str)-[r]-() return COUNT(r)\"\n",
    "    val cursor = Executor.execute(sc, query, Map((\"\",\"\")))\n",
    "    val response = cursor.rows.toList\n",
    "    row :+ response(0)(0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T10:04:53.443630Z",
     "start_time": "2019-04-26T09:52:31.097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data overwritten\n",
      "+-----------------+--------+---------+\n",
      "|            label| count_n|  count_r|\n",
      "+-----------------+--------+---------+\n",
      "|         HC_repay| 1963947|        0|\n",
      "|            Phone|22173769| 51227904|\n",
      "|      Mobilephone|18822274| 42658529|\n",
      "|          Address| 7976921| 18842999|\n",
      "|       HC_presona|    8279|  5504220|\n",
      "|     Bank_account| 1868242|  2801423|\n",
      "|        HC_client| 4112976| 70850813|\n",
      "|           Person|19342651|140069325|\n",
      "|      HC_contract| 2185195|  2180338|\n",
      "|          ID_card| 4194817|  9779910|\n",
      "|     HC_blacklist|   94850|        0|\n",
      "|         Landline| 3351495|  8569375|\n",
      "|          HC_role|     206|        0|\n",
      "|     HC_intopiece| 5308780| 80801782|\n",
      "|    HC_client_mes| 6216762| 10353541|\n",
      "|HC_intopiece_link|15040832|        0|\n",
      "|         HC_staff|  177505|   718550|\n",
      "|          Company| 2892023| 21315735|\n",
      "|            Email| 1137400|  1169363|\n",
      "|      HC_graylist|   55464|        0|\n",
      "+-----------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickleThis(sc, label_rel_counts, \"label count_n count_r\", \"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")\n",
    "val label_rel_counts_df = sqlContext.read.load(\"file:///home/jovyan/work/hadoop-client-scala-prod/pickles/label_rel_counts.pickle\")\n",
    "label_rel_counts_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T02:54:56.535724Z",
     "start_time": "2019-04-23T02:54:55.984Z"
    }
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:HC_role)-[r]->(m) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(10).batch(20)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T02:55:03.157058Z",
     "start_time": "2019-04-23T02:55:02.890Z"
    }
   },
   "outputs": [],
   "source": [
    "graph.vertices.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T09:55:45.091353Z",
     "start_time": "2019-04-15T09:55:44.935Z"
    }
   },
   "outputs": [],
   "source": [
    "// divided by 40 cores\n",
    "32697950/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T09:59:10.554552Z",
     "start_time": "2019-04-15T09:59:10.259Z"
    }
   },
   "outputs": [],
   "source": [
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(40).batch(850000)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:05:46.373913Z",
     "start_time": "2019-04-15T09:59:29.659Z"
    }
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.vertices.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:14:21.215679Z",
     "start_time": "2019-04-15T10:11:03.345Z"
    }
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.edges.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern is using `id(n)` if node property is set to null, like example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:41:11.666615Z",
     "start_time": "2019-04-15T10:41:10.757Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "// val g = Neo4jGraph.loadGraph(sc, \"Person\", Seq(\"Person_relation\"), \"Person\")\n",
    "val graph = neo.pattern((\"Person\",null),(\"Person_relation\",null),(\"Person\",null)).partitions(40).batch(850000).loadGraph[Long,Long]\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:26:46.994662Z",
     "start_time": "2019-04-15T10:18:01.603Z"
    }
   },
   "outputs": [],
   "source": [
    "// 1513901\n",
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.vertices.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:48:14.156199Z",
     "start_time": "2019-04-15T10:41:13.389Z"
    }
   },
   "outputs": [],
   "source": [
    "// 1513901\n",
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.edges.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// // Find the connected components\n",
    "// val g2 = g.connectedComponents()\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// g2.vertices.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// val return_val = Neo4jGraph.saveGraph(sc, g2, \"sdComponent\")\n",
    "// println(return_val) // how many nodes has been written\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//   // label1, label2, relTypes are optional\n",
    "//   // MATCH (n:${label(label1}})-[via:${rels(relTypes)}]->(m:${label(label2)}) RETURN id(n) as from, id(m) as to\n",
    "//   def loadGraph(sc: SparkContext, label1: String, relTypes: Seq[String], label2: String) : Graph[Any,Int] = {\n",
    "//     def label(l : String) = if (l == null) \"\" else \":`\"+l+\"`\"\n",
    "//     def rels(relTypes : Seq[String]) = relTypes.map(\":`\"+_+\"`\").mkString(\"|\")\n",
    "\n",
    "//     val relStmt = s\"MATCH (n${label(label1)})-[via${rels(relTypes)}]->(m${label(label2)}) RETURN id(n) as from, id(m) as to\"\n",
    "\n",
    "//     loadGraphFromNodePairs[Any](sc,relStmt)\n",
    "//   }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
