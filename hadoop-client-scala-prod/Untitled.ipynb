{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T03:42:20.642034Z",
     "start_time": "2019-05-07T03:42:20.165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:58:58.507338Z",
     "start_time": "2019-05-09T08:58:58.371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scala.Console$@6dd97bc5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Console.out.flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T03:12:16.219694Z",
     "start_time": "2019-05-07T03:12:15.848Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.lang.Exception\n",
    "import java.io.{File, FileWriter, IOException, Writer}\n",
    "import org.apache.spark.{SparkException}\n",
    "import scala.util.control.NonFatal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T03:42:21.755111Z",
     "start_time": "2019-05-07T03:42:21.524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Double = 1.0E8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val qq = 1e8\n",
    "qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T03:05:09.372813Z",
     "start_time": "2019-05-07T03:05:08.147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val qq = new SparkException(\"sd\")\n",
    "qq.isInstanceOf[java.lang.Exception]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T02:44:41.494532Z",
     "start_time": "2019-05-07T02:44:41.201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught hep\n"
     ]
    }
   ],
   "source": [
    "try {\n",
    "    throw new SparkException(\"hep\")\n",
    "} catch {\n",
    "    case e: java.lang.Exception => {\n",
    "    println(\"caught hep\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T02:37:11.062527Z",
     "start_time": "2019-05-07T02:37:10.567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.Exception\n",
       "Message: hep\n",
       "StackTrace: $line22.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)\n",
       "$line22.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:31)\n",
       "$line22.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:33)\n",
       "$line22.$read$$iwC$$iwC$$iwC.<init>(<console>:35)\n",
       "$line22.$read$$iwC$$iwC.<init>(<console>:37)\n",
       "$line22.$read$$iwC.<init>(<console>:39)\n",
       "$line22.$read.<init>(<console>:41)\n",
       "$line22.$read$.<init>(<console>:45)\n",
       "$line22.$read$.<clinit>(<console>)\n",
       "$line22.$eval$.<init>(<console>:7)\n",
       "$line22.$eval$.<clinit>(<console>)\n",
       "$line22.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "throw new java.lang.Exception(\"hep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T07:24:44.913104Z",
     "start_time": "2019-04-30T07:24:44.805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0E10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T07:20:25.894562Z",
     "start_time": "2019-04-30T07:20:25.545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "========================================sdSTARTprintStackTrace\n",
      "========================================\n",
      "java.lang.ArithmeticException: / by zero\n",
      "\tat $line58.$read$$iwC$$iwC$$iwC$$iwC.myf(<console>:21)\n",
      "\tat $line59.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply$mcVI$sp(<console>:27)\n",
      "\tat $line59.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:24)\n",
      "\tat $line59.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:24)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156)\n",
      "\tat $line59.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
      "\tat $line59.$read$$iwC$$iwC$$iwC.<init>(<console>:33)\n",
      "\tat $line59.$read$$iwC$$iwC.<init>(<console>:35)\n",
      "\tat $line59.$read$$iwC.<init>(<console>:37)\n",
      "\tat $line59.$read.<init>(<console>:39)\n",
      "\tat $line59.$read$.<init>(<console>:43)\n",
      "\tat $line59.$read$.<clinit>(<console>)\n",
      "\tat $line59.$eval$.<init>(<console>:7)\n",
      "\tat $line59.$eval$.<clinit>(<console>)\n",
      "\tat $line59.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
      "\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
      "\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
      "\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "========================================sdENDprintStackTrace\n",
      "========================================\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "val qqs:Array[Int] = Array(1,0,3)\n",
    "\n",
    "def myf(qq:Int) : Int = {\n",
    "try {\n",
    "  10/qq\n",
    "} catch {\n",
    "  case e: Exception => {\n",
    "      println(\"==\" * 20 ++ \"sdSTARTprintStackTrace\")\n",
    "      println(\"==\" * 20)\n",
    "      e.printStackTrace\n",
    "      println(\"==\" * 20 ++ \"sdENDprintStackTrace\")\n",
    "      println(\"==\" * 20)\n",
    "      0\n",
    "  }\n",
    "}\n",
    "}\n",
    "\n",
    "qqs.foreach( qq => {\n",
    "// var x:Int = null.asInstanceOf[Int]\n",
    "\n",
    "println(myf(qq))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T02:03:11.074517Z",
     "start_time": "2019-04-29T02:03:10.799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hep\n",
      "661\n"
     ]
    }
   ],
   "source": [
    "class Point(var x: Int, var y: Int) {\n",
    "  var num = null.asInstanceOf[Int]\n",
    "    \n",
    "  def myf() : Unit = {\n",
    "      this.num = 661\n",
    "      println(\"hep\")\n",
    "  }\n",
    "    \n",
    "  def move(dx: Int, dy: Int): Unit = {\n",
    "    x = x + dx\n",
    "    y = y + dy\n",
    "      println(this.num)\n",
    "  }\n",
    "\n",
    "  override def toString: String =\n",
    "    s\"($x, $y)\"\n",
    "}\n",
    "\n",
    "val point1 = new Point(2, 3)\n",
    "// point1.x  // 2\n",
    "// println(point1)  // prints (2, 3)\n",
    "\n",
    "point1.myf()\n",
    "point1.move(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T02:57:26.914361Z",
     "start_time": "2019-04-29T02:57:26.852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[_] = null"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null.asInstanceOf[Array[_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:16:12.097034Z",
     "start_time": "2019-04-30T08:16:08.646Z"
    }
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val df = Seq(\n",
    "    (\"first\", 2.0),\n",
    "    (\"test\", 1.5),\n",
    "    (\"choose\", 8.0)\n",
    "  ).toDF(\"id\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:20:56.589532Z",
     "start_time": "2019-04-30T08:20:56.531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.Row = [first,2.0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T08:21:01.773919Z",
     "start_time": "2019-04-30T08:21:01.611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.Row = [test,1.5]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter($\"id\" === \"test\").head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T08:13:23.305654Z",
     "start_time": "2019-04-26T08:13:23.171Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:22: error: value toDF is not a member of Seq[(String, Double)]\n",
       "possible cause: maybe a semicolon is missing before `value toDF'?\n",
       "         ).toDF(\"id\", \"val\")\n",
       "           ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply[T](t: T) = {\n",
    "    t match {\n",
    "  \n",
    "  case t: String => {\n",
    "       \"you gave me a String\"\n",
    "  }\n",
    "  case t: Array[Array[Int]]  => {\n",
    "       \"you gave me an AAAAArray Int\"\n",
    "  }\n",
    "  case t: Array[Int]  => \"you gave me an Array Int\"\n",
    "  case t: Array[String]  => \"you gave me an Array String\"\n",
    "  case t: Array[_]  => \"you gave me an Array\"\n",
    "  case _ => throw new Exception(\"Wrong argument type\")\n",
    "}\n",
    "}\n",
    "val qq:String = \"sdf\"\n",
    "// val qqq:Array[Int] = Array(1,2,3)\n",
    "// val qqqq:Array[String] = Array(\"1,2,3\")\n",
    "// val q:Double = 0.3\n",
    "val w = Array(3.4)\n",
    "// apply(w)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T06:07:16.310893Z",
     "start_time": "2019-04-24T06:07:12.936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+--------+--------+----+\n",
      "|    callId|oCallId|           callTime|duration|calltype|swId|\n",
      "+----------+-------+-------------------+--------+--------+----+\n",
      "|4580056797|      0|2015-07-29 10:38:42|       0|       1|   1|\n",
      "|4580056797|      0|2015-07-29 10:38:42|       0|       1|   1|\n",
      "+----------+-------+-------------------+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType};\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val rdd_original : Array[Array[String]] = Array(\n",
    "    Array(\"4580056797\", \"0\", \"2015-07-29 10:38:42\", \"0\", \"1\", \"1\"), \n",
    "    Array(\"4580056797\", \"0\", \"2015-07-29 10:38:42\", \"0\", \"1\", \"1\"))\n",
    "\n",
    "val rdd : List[Array[String]] = rdd_original.toList\n",
    "\n",
    "val schemaString = \"callId oCallId callTime duration calltype swId\"\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val schema =\n",
    "  StructType(\n",
    "    schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "// Convert records of the RDD to Rows.\n",
    "val rowRDD = rdd.map(p => Row(p: _*)) // using splat is easier\n",
    "// val rowRDD = rdd.map(p => Row(p(0), p(1), p(2), p(3), p(4), p(5))) // this also works\n",
    "\n",
    "val df = sqlContext.createDataFrame(sc.parallelize(rowRDD:List[Row]), schema)\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T06:40:39.889171Z",
     "start_time": "2019-04-24T06:40:39.647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[org.apache.spark.sql.Row] = Array([4580056797,0,2015-07-29 10:38:42,0,1,1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T03:13:09.641080Z",
     "start_time": "2019-04-24T03:13:08.185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List([Ljava.lang.String;@273b2374, [Ljava.lang.String;@801c4e4)\n",
      "StructType(StructField(name,StringType,true), StructField(age,StringType,true))\n",
      "List([ss,4], [sd,2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:82: error: overloaded method value createDataFrame with alternatives:\n",
       "  (data: java.util.List[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.api.java.JavaRDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rdd: org.apache.spark.rdd.RDD[_],beanClass: Class[_])org.apache.spark.sql.DataFrame <and>\n",
       "  (rows: java.util.List[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.api.java.JavaRDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame <and>\n",
       "  (rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.DataFrame\n",
       " cannot be applied to (scala.collection.immutable.List[org.apache.spark.sql.Row], org.apache.spark.sql.types.StructType)\n",
       "       val peopleDataFrame = sqlContext.createDataFrame(rowRDD:List[org.apache.spark.sql.Row], schema: org.apache.spark.sql.types.StructType)\n",
       "                                        ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "// Import Row.\n",
    "import org.apache.spark.sql.Row\n",
    "// Import Spark SQL data types\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType};\n",
    "\n",
    "val people : List[Array[String]] = List(Array(\"ss\",\"4\"),Array(\"sd\",\"2\"))\n",
    "println(people)\n",
    "\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val schema =\n",
    "  StructType(\n",
    "    schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "println(schema)\n",
    "\n",
    "// val rowRDD = rdd.map(p => Array(p(0), p(1), p(2),p(3),p(4),p(5).trim))\n",
    "// val rowRDD = rdd.map(p => Row(p(0), p(1), p(2),p(3),p(4),p(5).trim))\n",
    "\n",
    "// Convert records of the RDD (people) to Rows.\n",
    "val rowRDD = people.map(p => Row(p(0), p(1)))\n",
    "println(rowRDD)\n",
    "\n",
    "// val peopleDataFrame = sqlContext.createDataFrame(sc.parallelize(rowRDD:List[Row]), schema)\n",
    "val peopleDataFrame = sqlContext.createDataFrame(rowRDD:List[org.apache.spark.sql.Row], schema: org.apache.spark.sql.types.StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T03:12:13.293328Z",
     "start_time": "2019-04-24T03:12:13.123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[3] at parallelize at <console>:63"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(rowRDD:List[Row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T09:15:19.458532Z",
     "start_time": "2019-04-23T09:15:19.227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:23: error: no `: _*' annotation allowed here\n",
       "(such annotations are only allowed in arguments to *-parameters)\n",
       "              print(fruits: _*)\n",
       "                          ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// a sequence of strings\n",
    "val fruits = List(\"apple\", \"banana\", \"cherry\")\n",
    "\n",
    "// pass the sequence to the varargs field\n",
    "print(fruits: _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T05:08:11.983834Z",
     "start_time": "2019-04-23T05:08:11.674Z"
    }
   },
   "outputs": [],
   "source": [
    "import scala.reflect.runtime.universe._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T02:46:20.597696Z",
     "start_time": "2019-04-23T02:46:18.570Z"
    }
   },
   "outputs": [],
   "source": [
    "object Tabulator {\n",
    "  def format(table: Seq[Seq[Any]]) = table match {\n",
    "    case Seq() => \"\"\n",
    "    case _ => \n",
    "      val sizes = for (row <- table) yield (for (cell <- row) yield if (cell == null) 0 else cell.toString.length)\n",
    "      val colSizes = for (col <- sizes.transpose) yield col.max\n",
    "      val rows = for (row <- table) yield formatRow(row, colSizes)\n",
    "      formatRows(rowSeparator(colSizes), rows)\n",
    "  }\n",
    "\n",
    "  def formatRows(rowSeparator: String, rows: Seq[String]): String = (\n",
    "    rowSeparator :: \n",
    "    rows.head :: \n",
    "    rowSeparator :: \n",
    "    rows.tail.toList ::: \n",
    "    rowSeparator :: \n",
    "    List()).mkString(\"\\n\")\n",
    "\n",
    "  def formatRow(row: Seq[Any], colSizes: Seq[Int]) = {\n",
    "    val cells = (for ((item, size) <- row.zip(colSizes)) yield if (size == 0) \"\" else (\"%\" + size + \"s\").format(item))\n",
    "    cells.mkString(\"|\", \"|\", \"|\")\n",
    "  }\n",
    "\n",
    "  def rowSeparator(colSizes: Seq[Int]) = colSizes map { \"-\" * _ } mkString(\"+\", \"+\", \"+\")\n",
    "    \n",
    "  def format(table: Seq[Array[Any]])(implicit d: DummyImplicit) : String = {\n",
    "      format(table.map(_.toSeq))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T02:46:20.809352Z",
     "start_time": "2019-04-23T02:46:19.226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val qq:Seq[Array[Any]] = List(Array(\"head1\", \"head2\", \"head3\"), Array(\"one\", \"two\", \"three\"))\n",
    "// qq.map(_.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T02:46:20.957515Z",
     "start_time": "2019-04-23T02:46:20.050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+-----+-----+\n",
       "|head1|head2|head3|\n",
       "+-----+-----+-----+\n",
       "|  one|  two|three|\n",
       "+-----+-----+-----+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tabulator.format(qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T09:59:44.665449Z",
     "start_time": "2019-04-22T09:59:43.864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (n:`label1`)-[via]->(m) RETURN id(n) as from, id(m) as to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  // label1, label2, relTypes are optional\n",
    "  // MATCH (n:${name(label1}})-[via:${rels(relTypes)}]->(m:${name(label2)}) RETURN id(n) as from, id(m) as to\n",
    "val label1:String = \"label1\"\n",
    "// val relTypes: Seq[String] = Seq(\"r1\",\"r2\")\n",
    "val relTypes: Seq[String] = Seq.empty\n",
    "val label2: String = null\n",
    "//   def loadGraph(sc: SparkContext, label1: String, relTypes: Seq[String], label2: String) : Graph[Any,Int] = {\n",
    "def label(l : String) = if (l == null) \"\" else \":`\"+l+\"`\"\n",
    "def rels(relTypes : Seq[String]) = relTypes.map(\":`\"+_+\"`\").mkString(\"|\")\n",
    "\n",
    "val relStmt = s\"MATCH (n${label(label1)})-[via${rels(relTypes)}]->(m${label(label2)}) RETURN id(n) as from, id(m) as to\"\n",
    "println(relStmt)\n",
    "//    loadGraphFromNodePairs[Any](sc,relStmt)\n",
    "//   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T09:45:05.441788Z",
     "start_time": "2019-04-16T09:45:05.002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(a, number, of, words)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val it = Iterator(\"a\", \"number\", \"of\", \"words\")\n",
    "val mycollection = it.toList\n",
    "mycollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:12:50.959176Z",
     "start_time": "2019-04-18T02:12:50.223Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T03:19:02.309292Z",
     "start_time": "2019-04-16T03:19:02.084Z"
    }
   },
   "outputs": [],
   "source": [
    "val neo = Neo4j(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:55:12.457695Z",
     "start_time": "2019-04-15T07:55:12.168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  List(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1,2,3) match {\n",
    "      case n :+ ns => println(ns + \"  \"+  n)\n",
    "      case Nil     => 666\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:58:52.296314Z",
     "start_time": "2019-04-15T07:58:52.025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(3)  2  1\n"
     ]
    }
   ],
   "source": [
    "List(1,2,3) match {\n",
    "      case n0 :: n1 :: ns => println(ns + \"  \"+  n1 + \"  \" + n0)\n",
    "      case Nil     => 666\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T06:49:04.986629Z",
     "start_time": "2019-04-15T06:49:04.867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Some(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:32:08.663125Z",
     "start_time": "2019-04-15T03:32:06.480Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "  case class NameProp(name:String, property:String = null) {\n",
    "    def this(tuple : (String,String)) = this(tuple._1, tuple._2)\n",
    "    def asTuple = (name,property)\n",
    "  }\n",
    "\n",
    "  case class Pattern(source:NameProp, edges:Seq[NameProp], target:NameProp) {\n",
    "    private def quote(s:String):String = \"`\"+s+\"`\"\n",
    "    private def relTypes = \":\" + edges.map(\"`\" + _.name + \"`\").mkString(\":\")\n",
    "\n",
    "    // fast count-queries for the partition sizes\n",
    "    def countNode(node:NameProp) = s\"MATCH (:`${node.name}`) RETURN count(*) as total\"\n",
    "    def countRelsSource(rel: NameProp) = s\"MATCH (:`${source.name}`)-[:`${rel.name}`]->() RETURN count(*)\"\n",
    "    def countRelsTarget(rel: NameProp) = s\"MATCH ()-[:`${rel.name}`]->(:`${target.name}`) RETURN count(*) AS total\"\n",
    "\n",
    "    def nodeQueries = List(nodeQuery(source),nodeQuery(target))\n",
    "    def relQueries = edges.map(relQuery)\n",
    "\n",
    "    def relQuery(rel : NameProp) = {\n",
    "      val c: List[String] = List(countRelsSource(rel), countRelsTarget(rel))\n",
    "      var q = s\"MATCH (n:`${source.name}`)-[rel:`${rel.name}`]->(m:`${target.name}`) WITH n,rel,m SKIP {_skip} LIMIT {_limit} RETURN id(n) as src, id(m) as dst \"\n",
    "      if (rel.property != null) (q + s\", rel.`${rel.property}` as value\", c)\n",
    "      else (q, c)\n",
    "    }\n",
    "    def nodeQuery(node: NameProp) = {\n",
    "      var c = countNode(node)\n",
    "      var q : String = s\"MATCH (n:`${node.name}`) WITH n SKIP {_skip} LIMIT {_limit} RETURN id(n) AS id\"\n",
    "      if (node.property != null) (q + s\", n.`${node.property}` as value\",c)\n",
    "      else (q,c)\n",
    "    }\n",
    "    def this(source:(String,String), edges: Seq[(String,String)], target: (String,String)) =\n",
    "      this(new NameProp(source), edges.map(new NameProp(_)), new NameProp(target))\n",
    "    def this(source:String, edges: Seq[String], target: String) =\n",
    "      this(NameProp(source), edges.map(NameProp(_)), NameProp(target))\n",
    "    def edgeNames = edges.map(_.name)\n",
    "  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:26:19.828963Z",
     "start_time": "2019-04-15T03:26:19.656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(1, 2, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:32:12.238604Z",
     "start_time": "2019-04-15T03:32:11.983Z"
    }
   },
   "outputs": [],
   "source": [
    "val qq = new Pattern(new NameProp(\"n1\",\"p1\"),Seq(new NameProp(\"r1\",\"rp1\"),new NameProp(\"r2\",\"rp2\")),new NameProp(\"n2\",\"p2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:32:29.697689Z",
     "start_time": "2019-04-15T03:32:29.532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((MATCH (n:`n1`)-[rel:`r1`]->(m:`n2`) WITH n,rel,m SKIP {_skip} LIMIT {_limit} RETURN id(n) as src, id(m) as dst , rel.`rp1` as value,List(MATCH (:`n1`)-[:`r1`]->() RETURN count(*), MATCH ()-[:`r1`]->(:`n2`) RETURN count(*) AS total)), (MATCH (n:`n1`)-[rel:`r2`]->(m:`n2`) WITH n,rel,m SKIP {_skip} LIMIT {_limit} RETURN id(n) as src, id(m) as dst , rel.`rp2` as value,List(MATCH (:`n1`)-[:`r2`]->() RETURN count(*), MATCH ()-[:`r2`]->(:`n2`) RETURN count(*) AS total)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.relQueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:34:09.292487Z",
     "start_time": "2019-04-15T03:34:09.103Z"
    }
   },
   "outputs": [],
   "source": [
    "val qq = new Pattern(new NameProp(\"n1\",\"p1\"),Seq(new NameProp(\"r1\",\"rp1\")),new NameProp(\"n2\",\"p2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:34:13.276678Z",
     "start_time": "2019-04-15T03:34:13.112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((MATCH (n:`n1`)-[rel:`r1`]->(m:`n2`) WITH n,rel,m SKIP {_skip} LIMIT {_limit} RETURN id(n) as src, id(m) as dst , rel.`rp1` as value,List(MATCH (:`n1`)-[:`r1`]->() RETURN count(*), MATCH ()-[:`r1`]->(:`n2`) RETURN count(*) AS total)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.relQueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T06:13:30.244867Z",
     "start_time": "2019-04-15T06:13:29.980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((MATCH (n:`n1`) WITH n SKIP {_skip} LIMIT {_limit} RETURN id(n) AS id, n.`p1` as value,MATCH (:`n1`) RETURN count(*) as total), (MATCH (n:`n2`) WITH n SKIP {_skip} LIMIT {_limit} RETURN id(n) AS id, n.`p2` as value,MATCH (:`n2`) RETURN count(*) as total))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.nodeQueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
