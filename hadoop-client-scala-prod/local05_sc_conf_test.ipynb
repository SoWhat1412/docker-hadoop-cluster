{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.100.34.19:9988 neo4j, testing w full data, around 75g\n",
    "from 03, adjusting spark resources and testing with different amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:19:55.203636Z",
     "start_time": "2019-04-16T06:19:54.465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showtypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:19:56.540396Z",
     "start_time": "2019-04-16T06:19:55.878Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://10.100.34.19:4040/\n",
      "(spark.executor.instances,3)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.repl.class.uri,http://10.100.34.19:9996)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.memory,5g)\n",
      "(spark.driver.extraJavaOptions,-Xms1024M -Xmx4096M -Dlog4j.logLevel=info)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.app.id,local-1555395526091)\n",
      "(spark.master,local[20])\n",
      "(spark.neo4j.bolt.user,neo4j)\n",
      "(spark.executor.port,9993)\n",
      "(spark.executor.cores,6)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.driver.port,9992)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.driver.blockManager.port,9990)\n",
      "(spark.externalBlockStore.folderName,spark-513b3031-bfe3-4048-80ed-80d03a238fa2)\n"
     ]
    }
   ],
   "source": [
    "println(\"http://10.100.34.19:4040/\") //spark web UI\n",
    "sc.getConf.getAll.foreach(println) // %lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:19:58.897420Z",
     "start_time": "2019-04-16T06:19:57.285Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.neo4j.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:07.591825Z",
     "start_time": "2019-04-16T06:20:05.527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "val neo = Neo4j(sc)\n",
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(5).batch(10)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]\n",
    "println(graph.vertices.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:13.048162Z",
     "start_time": "2019-04-16T06:20:12.628Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:14.623507Z",
     "start_time": "2019-04-16T06:20:13.724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NullPointerException\n",
       "Message: null\n",
       "StackTrace: org.apache.spark.rdd.ShuffledRDD.getPreferredLocations(ShuffledRDD.scala:90)\n",
       "org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply(RDD.scala:257)\n",
       "org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply(RDD.scala:257)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.preferredLocations(RDD.scala:256)\n",
       "org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$2$$anonfun$2.apply(ZippedPartitionsRDD.scala:60)\n",
       "org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$2$$anonfun$2.apply(ZippedPartitionsRDD.scala:60)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n",
       "scala.collection.immutable.List.foreach(List.scala:318)\n",
       "scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n",
       "scala.collection.AbstractTraversable.map(Traversable.scala:105)\n",
       "org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$2.apply(ZippedPartitionsRDD.scala:60)\n",
       "org.apache.spark.rdd.ZippedPartitionsBaseRDD$$anonfun$getPartitions$2.apply(ZippedPartitionsRDD.scala:59)\n",
       "scala.Array$.tabulate(Array.scala:331)\n",
       "org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:59)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:48)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:50)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:52)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:54)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:56)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:58)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:60)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:62)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:64)\n",
       "$line55.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
       "$line55.$read$$iwC$$iwC$$iwC.<init>(<console>:68)\n",
       "$line55.$read$$iwC$$iwC.<init>(<console>:70)\n",
       "$line55.$read$$iwC.<init>(<console>:72)\n",
       "$line55.$read.<init>(<console>:74)\n",
       "$line55.$read$.<init>(<console>:78)\n",
       "$line55.$read$.<clinit>(<console>)\n",
       "$line55.$eval$.<init>(<console>:7)\n",
       "$line55.$eval$.<clinit>(<console>)\n",
       "$line55.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val neo = Neo4j(sc)\n",
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(5).batch(10)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]\n",
    "println(graph.vertices.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:17.661785Z",
     "start_time": "2019-04-16T06:20:16.740Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.executor.instances,3)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.memory,5g)\n",
      "(spark.driver.extraJavaOptions,-Xms1024M -Xmx4096M -Dlog4j.logLevel=info)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.neo4j.bolt.user,xxx)\n",
      "(spark.executor.cores,6)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.executor.port,9993)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.app.id,local-1555395617453)\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.master,local[40])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.driver.port,9992)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.externalBlockStore.folderName,spark-54b0ac57-57fd-44f1-9673-c830803dd1c6)\n",
      "(spark.driver.blockManager.port,9990)\n",
      "(spark.app.name,org.apache.toree.Main)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "val conf = new SparkConf().set(\"spark.master\",\"local[40]\").set(\"spark.neo4j.bolt.user\",\"xxx\")\n",
    "val sc = new SparkContext(conf)\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:21.062893Z",
     "start_time": "2019-04-16T06:20:19.972Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 3 in stage 1.0 failed 1 times, most recent failure: Lost task 3.0 in stage 1.0 (TID 8, localhost): org.neo4j.driver.v1.exceptions.AuthenticationException: The client is unauthorized due to authentication failure.\n",
       "\tat org.neo4j.driver.internal.net.SocketResponseHandler.handleFailureMessage(SocketResponseHandler.java:71)\n",
       "\tat org.neo4j.driver.internal.messaging.PackStreamMessageFormatV1$Reader.unpackFailureMessage(PackStreamMessageFormatV1.java:457)\n",
       "\tat org.neo4j.driver.internal.messaging.PackStreamMessageFormatV1$Reader.read(PackStreamMessageFormatV1.java:418)\n",
       "\tat org.neo4j.driver.internal.net.SocketClient.receiveOne(SocketClient.java:176)\n",
       "\tat org.neo4j.driver.internal.net.SocketClient.receiveAll(SocketClient.java:170)\n",
       "\tat org.neo4j.driver.internal.net.SocketConnection.receiveAll(SocketConnection.java:198)\n",
       "\tat org.neo4j.driver.internal.net.SocketConnection.sync(SocketConnection.java:153)\n",
       "\tat org.neo4j.driver.internal.net.SocketConnection.init(SocketConnection.java:115)\n",
       "\tat org.neo4j.driver.internal.net.ConcurrencyGuardingConnection.init(ConcurrencyGuardingConnection.java:52)\n",
       "\tat org.neo4j.driver.internal.net.SocketConnector.connect(SocketConnector.java:58)\n",
       "\tat org.neo4j.driver.internal.net.pooling.SocketConnectionPool$ConnectionSupplier.get(SocketConnectionPool.java:204)\n",
       "\tat org.neo4j.driver.internal.net.pooling.SocketConnectionPool$ConnectionSupplier.get(SocketConnectionPool.java:186)\n",
       "\tat org.neo4j.driver.internal.net.pooling.BlockingPooledConnectionQueue.acquire(BlockingPooledConnectionQueue.java:93)\n",
       "\tat org.neo4j.driver.internal.net.pooling.SocketConnectionPool.acquireConnection(SocketConnectionPool.java:137)\n",
       "\tat org.neo4j.driver.internal.net.pooling.SocketConnectionPool.acquire(SocketConnectionPool.java:76)\n",
       "\tat org.neo4j.driver.internal.DirectConnectionProvider.acquireConnection(DirectConnectionProvider.java:45)\n",
       "\tat org.neo4j.driver.internal.NetworkSession.acquireConnection(NetworkSession.java:347)\n",
       "\tat org.neo4j.driver.internal.NetworkSession.run(NetworkSession.java:103)\n",
       "\tat org.neo4j.driver.internal.NetworkSession.run(NetworkSession.java:93)\n",
       "\tat org.neo4j.driver.internal.NetworkSession.run(NetworkSession.java:80)\n",
       "\tat org.neo4j.spark.Executor$.execute(Neo4j.scala:384)\n",
       "\tat org.neo4j.spark.Neo4jRDD.compute(Neo4j.scala:430)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:50)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:52)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:54)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:56)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:58)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:60)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:62)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:64)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
       "$line67.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:68)\n",
       "$line67.$read$$iwC$$iwC$$iwC.<init>(<console>:70)\n",
       "$line67.$read$$iwC$$iwC.<init>(<console>:72)\n",
       "$line67.$read$$iwC.<init>(<console>:74)\n",
       "$line67.$read.<init>(<console>:76)\n",
       "$line67.$read$.<init>(<console>:80)\n",
       "$line67.$read$.<clinit>(<console>)\n",
       "$line67.$eval$.<init>(<console>:7)\n",
       "$line67.$eval$.<clinit>(<console>)\n",
       "$line67.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val neo = Neo4j(sc)\n",
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(5).batch(10)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]\n",
    "println(graph.vertices.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:20:54.910990Z",
     "start_time": "2019-04-16T06:20:53.948Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.app.id,local-1555395654691)\n",
      "(spark.executor.instances,3)\n",
      "(spark.neo4j.bolt.url,bolt://neo4j:neo4j0fcredithc@10.100.34.19:9989)\n",
      "(spark.yarn.queue,kg)\n",
      "(spark.driver.extraClassPath,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.memory,5g)\n",
      "(spark.driver.extraJavaOptions,-Xms1024M -Xmx4096M -Dlog4j.logLevel=info)\n",
      "(spark.neo4j.bolt.password,neo4j0fcredithc)\n",
      "(spark.driver.host,10.100.34.19)\n",
      "(spark.neo4j.bolt.user,neo4j)\n",
      "(spark.executor.cores,6)\n",
      "(spark.fileserver.port,9994)\n",
      "(spark.externalBlockStore.folderName,spark-701c8682-ff84-48cc-b7e3-1662526134e4)\n",
      "(spark.executor.port,9993)\n",
      "(spark.jars,file:///usr/local/spark/jars-sd/neo4j-spark-connector-full-2.0.0-M2-s_1.6.1.jar,file:///usr/local/spark/jars-sd/netty-all-4.1.8.Final.jar,file:///usr/local/spark/jars-sd/neo4j-java-driver-1.7.2.jar,file:///usr/local/spark/jars-sd/graphframes-0.5.0-spark1.6-s_2.10.jar,file:///usr/local/spark/jars-sd/scala-logging-api_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-logging-slf4j_2.10-2.1.2.jar,file:///usr/local/spark/jars-sd/scala-reflect-2.10.4.jar,file:///usr/local/spark/jars-sd/slf4j-api-1.7.7.jar,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.1.0-incubating.jar)\n",
      "(spark.replClassServer.port,9995)\n",
      "(spark.master,local[40])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.driver.port,9992)\n",
      "(spark.broadcast.port,9991)\n",
      "(spark.driver.blockManager.port,9990)\n",
      "(spark.app.name,org.apache.toree.Main)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "val conf = new SparkConf().set(\"spark.master\",\"local[40]\").set(\"spark.neo4j.bolt.user\",\"neo4j\")\n",
    "val sc = new SparkContext(conf)\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T06:21:03.583043Z",
     "start_time": "2019-04-16T06:21:02.767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "val neo = Neo4j(sc)\n",
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(5).batch(10)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]\n",
    "println(graph.vertices.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T09:58:24.815442Z",
     "start_time": "2019-04-15T09:58:24.229Z"
    }
   },
   "outputs": [],
   "source": [
    "// get max count for partitions and batch\n",
    "\n",
    "val response = Executor.execute(sc, \"MATCH (n:Person) RETURN count(n)\", Map((\"\",\"\")))\n",
    "// response.rows.next()\n",
    "response.rows.foreach(el => println(el(0)))\n",
    "\n",
    "val response = Executor.execute(sc, \"MATCH p=()-[r:Person_relation]->() RETURN count(r)\", Map((\"\",\"\")))\n",
    "// response.rows.next()\n",
    "response.rows.foreach(el => println(el(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T09:55:45.091353Z",
     "start_time": "2019-04-15T09:55:44.935Z"
    }
   },
   "outputs": [],
   "source": [
    "// divided by 40 cores\n",
    "32697950/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T09:59:10.554552Z",
     "start_time": "2019-04-15T09:59:10.259Z"
    }
   },
   "outputs": [],
   "source": [
    "val neo4j: Neo4j = neo.rels(\"MATCH (n:Person)-[r:Person_relation]->(m:Person) RETURN id(n) as src, id(m) as dst, type(r) as value SKIP {_skip} LIMIT {_limit}\").partitions(40).batch(850000)\n",
    "val graph: Graph[Long, String] = neo4j.loadGraph[Long,String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:05:46.373913Z",
     "start_time": "2019-04-15T09:59:29.659Z"
    }
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.vertices.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:14:21.215679Z",
     "start_time": "2019-04-15T10:11:03.345Z"
    }
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.edges.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern is using `id(n)` if node property is set to null, like example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:41:11.666615Z",
     "start_time": "2019-04-15T10:41:10.757Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "// val g = Neo4jGraph.loadGraph(sc, \"Person\", Seq(\"Person_relation\"), \"Person\")\n",
    "val graph = neo.pattern((\"Person\",null),(\"Person_relation\",null),(\"Person\",null)).partitions(40).batch(850000).loadGraph[Long,Long]\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:26:46.994662Z",
     "start_time": "2019-04-15T10:18:01.603Z"
    }
   },
   "outputs": [],
   "source": [
    "// 1513901\n",
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.vertices.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T10:48:14.156199Z",
     "start_time": "2019-04-15T10:41:13.389Z"
    }
   },
   "outputs": [],
   "source": [
    "// 1513901\n",
    "val t_start = System.currentTimeMillis()\n",
    "\n",
    "println(graph.edges.count)\n",
    "\n",
    "val t_end = System.currentTimeMillis()\n",
    "println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// // Find the connected components\n",
    "// val g2 = g.connectedComponents()\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// g2.vertices.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val t_start = System.currentTimeMillis()\n",
    "\n",
    "// val return_val = Neo4jGraph.saveGraph(sc, g2, \"sdComponent\")\n",
    "// println(return_val) // how many nodes has been written\n",
    "\n",
    "// val t_end = System.currentTimeMillis()\n",
    "// println(\"Elapsed time: \" + (t_end - t_start)/1000d + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//   // label1, label2, relTypes are optional\n",
    "//   // MATCH (n:${label(label1}})-[via:${rels(relTypes)}]->(m:${label(label2)}) RETURN id(n) as from, id(m) as to\n",
    "//   def loadGraph(sc: SparkContext, label1: String, relTypes: Seq[String], label2: String) : Graph[Any,Int] = {\n",
    "//     def label(l : String) = if (l == null) \"\" else \":`\"+l+\"`\"\n",
    "//     def rels(relTypes : Seq[String]) = relTypes.map(\":`\"+_+\"`\").mkString(\"|\")\n",
    "\n",
    "//     val relStmt = s\"MATCH (n${label(label1)})-[via${rels(relTypes)}]->(m${label(label2)}) RETURN id(n) as from, id(m) as to\"\n",
    "\n",
    "//     loadGraphFromNodePairs[Any](sc,relStmt)\n",
    "//   }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree - Scala (local[20])",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
